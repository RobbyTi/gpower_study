---
title: "Tables for G*Power manuscript"
author: "Robert Thibault"
date: "`r Sys.Date()`"
output:
  pdf_document: default
header-includes: 
  - \usepackage[labelformat=empty]{caption}
  - \pagestyle{empty}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE) # This option stops the code chunks from being output from the knit
set.seed(1313) # Set a seed for Monte Carlo simulations
```

```{r packages}
library(tidyverse) # For cleaner code
library(knitr) # For kable function
library(kableExtra) # For kable table styling
library(irr) # For inter-rater agreement calculations
```

```{r clean}
# Load the raw data
filepath <- paste0(getwd(), "/../data/main_raw.csv" )
raw <- read.csv(filepath, header = TRUE)
df <- raw

df <- df %>% 
  # Remove the text column for identifying coders other than RTT, EZ, and HP (in the end, we only had these 3 coders)
  select(-coder_text) %>% 
  # Filter out the rows labeled "resolveNotes", in which we took some notes when resolving differences in coding in Google sheets.
  filter(coder != "resolveNotes") %>% 
  filter(id != "") %>%   # Remove two irrelevant rows that Qualtrics exports
  filter(id != "156") %>%  # Remove extra paper RTT coded after we had reached our sample size, but that no one else coded.
  
  # Recode the $power_text column to 0.80, where relevant. This is necessary as we updated the Qualtrics extraction form to add a button for "Yes (0.80)" to speed up coding
  mutate(
    power_text = if_else(power == "Yes (0.80)", "0.80", power_text),
    power = if_else(power == "Yes (0.80)", "Yes", power),
    # repeat for $alpha column
    alpha_text = if_else(alpha == "Yes (0.05)", "0.05", alpha_text),
    alpha = if_else(alpha == "Yes (0.05)", "Yes", alpha)
  )

# Recode Yes/No columns to 1 (yes), 0 (no), and NA
df <- 
  df %>% 
  mutate_at(c("version", "power", "alpha", "sample_size", "effect_size_value"),
      ~ case_when(
      . == "Yes" ~ 1L,
      . == "No" ~ 0L,
      is.na(.) ~ NA_integer_) 
      ) %>% 
  mutate(effect_size_type_binary = 
      case_when(
      effect_size_type == "No" ~ 0L,
      effect_size_type == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  stat_test_binary = 
      case_when(
      stat_test == "the statistical test is NOT reported" ~ 0L,
      stat_test == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  # Make new column that checks how many sample size calculations include all 6 elements
  all6 = (
    power + alpha + sample_size + effect_size_type_binary + effect_size_value + stat_test_binary
    ),
  # Recode some columns to binary that weren't previously only "Yes" and "No"
  all6_binary = 
      case_when(
      all6 == 6 ~ 1L,
      all6 < 6 ~ 0L,
      is.na(all6) ~ NA_integer_
      ),
  reproducible_binary = 
      case_when(
      grepl("Yes", reproducible) ~ 1L,
      reproducible == "No" ~ 0L,
      is.na(reproducible) ~ NA_integer_
      ),
  justification_binary = 
      case_when(
      grepl("No justification", justification) ~ 0L,
      is.na(justification) ~ NA_integer_,
      TRUE ~ 1L,
      )
  )

# Recode a few variables we changed part way through data collecting
df <- df %>% 
  mutate(anova_within_between = 
      case_when(
      grepl("the default option without accounting", anova_within_between) ~ "Yes, but the researchers use the default option.",
      grepl("Yes, and", anova_within_between) ~ "Yes, and the researchers selected a non-default option.",
      TRUE ~ as.character(anova_within_between)  # Keep all other values as they are
      )
  ) %>% 
  mutate(error = 
      case_when(
      grepl("Likely", error) ~ "Unsure (there's not enough information to reasonably code yes or no).",
      TRUE ~ as.character(error)  # Keep all other values as they are
      )
  ) %>% 
  mutate(participants = 
      case_when(
      grepl("Humans", participants) ~ "Human",
      TRUE ~ "Non-human animal"
      )
  )


# Recode a few columns to numerics
df$power_text <- as.numeric(df$power_text)
df$alpha_text <- as.numeric(df$alpha_text)
df$sample_size_text <- as.numeric(df$sample_size_text)
df$effect_size_value_text <- as.numeric(df$effect_size_value_text)
df$impact_factor <- as.numeric(df$impact_factor)

# Decompose the justifications variable
df$just_previous <- ifelse(grepl("Previous", df$justification), 1, 0)
df$just_pilot <- ifelse(grepl("pilot data", df$justification), 1, 0)
df$just_convention <- ifelse(grepl("Convention", df$justification), 1, 0)
df$just_mcid <- ifelse(grepl("MCID", df$justification), 1, 0)
df$just_none <- ifelse(grepl("No justification", df$justification), 1, 0)
df$just_ref <- ifelse(grepl("Reference to other study", df$justification), 1, 0)
df$just_other <- ifelse(grepl("Other", df$justification), 1, 0)

# Make a new dataframe that only includes the resolved coding. We do this by selecting coders with "/" in the name, because these represent two coders (e.g., "RTT/EZ")
df1_all <- df %>% filter(grepl("/", coder))

# Filter for articles that use GPower to solve for sample size
df1 <- df1_all %>% filter(grepl("Solves for sample size", power_calc_type))

# remove blank columns
df1 <- df1 %>% 
  select(-c(coder_number, participants_text, power_calc_type_text)) %>% 
  select(-stats_knowledge) # And this column because the values were for individual coders and we did not copy them into the resolved coding in a systematic way

filename <- paste0(getwd(), "/../data/main_clean.csv")
write_csv(df1, filename)
```

```{r functions}

# Create function to calculate the point estimates and confidence intervals based on Monte Carlo sampling
ci_calc <- function(num, denom){ 
     sim <- rbeta(100000, shape1=num, shape2=(denom-num)) # simulate the distribution
     point <- (quantile(sim, probs=0.5)) # calculate the point estimate and 95% CI bounds
     lb <- (quantile(sim, probs=0.025)) # lower bound
     ub <- (quantile(sim, probs=0.975)) # upper bound
     cis <- c(point, lb, ub) # confidence intervals
  return(cis)
}

# Create function to create point estimates and confidence intervals as they'll appear in the table output
ci_clean <- function(ci){
  ci <- ci %>% round(2)
  ci_out <- paste0(ci[1], " (", ci[2], " - ", ci[3], ")")
  return(ci_out)
}

ci_percent <- function(ci){
  ci <- ci %>% round(2) * 100
  if(ci[2] ==0){ci[2] <- "<1"}
  if(ci[3] ==0){ci[3] <- "<1"}
  ci_out <- paste0("(", ci[2], " - ", ci[3], "%)")
  return(ci_out)
}

point_percent <- function(ci){
  ci <- ci %>% round(2) * 100
  ci_out <- paste0(ci[1], "%") %>% 
  return(ci_out)
}

# Create function to calculate the point estimates and confidence intervals based on Monte Carlo sampling. This function combines to proportions.  
ci_calc2 <- function(num1, denom1, num2, denom2){ 
     sim1 <- rbeta(100000, shape1=num1, shape2=(denom1-num1)) # simulate the distribution
     sim2 <- rbeta(100000, shape1=num2, shape2=(denom2-num2)) # simulate the distribution
     sim <- sim1 * sim2
     point <- (quantile(sim, probs=0.5)) # calculate the point estimate and 95% CI bounds
     lb <- (quantile(sim, probs=0.025)) # lower bound
     ub <- (quantile(sim, probs=0.975)) # upper bound
     cis <- c(point, lb, ub) # confidence intervals
  return(cis)
}



```

```{r n_articles}
# This chunk estimates the total number of published articles that use GPower for doing (1) any power calculation, (2) an a priori sample size calculation, or (3) an a priori sample size calculation for an ANOVA

denom_all <- nrow(df1_all) # number of articles coded
denom <- nrow(df1) # number or articles with a sample size calc
n_included <- sum(df1_all$include == "Include")  # number of articles with any power calc
n_anova <- sum(grepl("Yes", df1$anova_within_between)) # number of articles with a power calc that solves for sample size for a relevant ANOVA
pmc_total = 3285893 # total number of PMC articles published in the time of our query: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pmc_hits = 22188  # number of search hits from our query (GPower OR “G Power”) AND ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pubmed_total = 7318980 # total number of PubMed articles published in that period: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [Date - Entry] : "2022/05/31" [Date - Entry])
proportion <- pmc_hits/pmc_total # proportion of PMC articles that were hits
mult_fact <- pubmed_total/pmc_total # the factor to multiply our findings for PMC articles to get an estimate for all PubMed articles.

# Initialize dataframe for point estimates and confidence intervals
cis <- data.frame(matrix(nrow=6, ncol=1))

# Run the function to calculate point estimates and confidence intervals 
cis <- as.data.frame(
  bind_cols(ci_calc(n_included, denom_all),
            ci_calc(denom, denom_all),
            ci_calc(n_anova, denom_all)
           )
)

# Calculate cis for the proportion of publications that use G*Power (output is in percentages)
cis_prop_any <- (ci_calc2(pmc_hits, pmc_total, n_included, denom_all) * 100) %>% 
  ci_clean()

# Calculate cis for the proportion of publications that use G*Power (output is in percentages)
cis_prop_sample_size <- (ci_calc2(pmc_hits, pmc_total, denom, denom_all) * 100) %>% 
  ci_clean()

# Calculate cis for PMC database
cis_pmc <- (cis * pmc_total * proportion) %>% 
  round(-3) %>% 
  summarise_all(ci_clean) %>% 
  t() %>% 
  as.data.frame()

# Calculate cis for PubMed database
cis_pubmed <- (cis * pubmed_total * proportion) %>% 
  round(-3) %>% 
  summarise_all(ci_clean) %>% 
  t() %>% 
  as.data.frame()


cis_out <- bind_cols(cis_pmc, cis_pubmed)

rownames(cis_out) <- c("Any power calculation",
                       "Sample size calculation",
                       "ANOVA sample size calculation"
                       )

colnames(cis_out) <- c("PubMed Central (95% CI)",
                       "PubMed (95% CI)"
                       )

n_per_any <- 1 / (proportion * (n_included / denom_all))
n_per_sample_size <- 1 / (proportion * (denom/ denom_all)) 

```

```{r calc_type}

# Calculate how many articles report solving for each of these elements
calc_type <- data.frame(
  type = c("Sample  size",
           "Power",
           "Effect size",
           "Sample size (after completing the study)",
           "Unsure"
  ),
  Freq = c(sum(grepl("sample size", df1_all$power_calc_type)),
           sum(grepl("power", df1_all$power_calc_type)),
           sum(grepl("effect size", df1_all$power_calc_type)),
           sum(grepl("Other", df1_all$power_calc_type)),
           sum(grepl("Unsure", df1_all$power_calc_type))
  )
) %>% 
# Calculate percentage and confidence intervals
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, n_included))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, n_included))) %>% 
  ungroup()

colnames(calc_type) <- c("Power calculation solved for:",
                         paste0("N=", n_included),
                         "Percent", 
                         "(95% CI)"
)

```

```{r reproducibility}
# Recode columns to have fewer factors that will fit more cleanly into a table
df1 <- df1 %>% 
  mutate(reproducible_factor = 
           case_when(
             reproducible == "Yes, based solely on the information in the article or its supplementary material" ~ "Yes, without assumptions",
             reproducible == "Yes, but I've had to make some assumptions. (please list the assumptions you made)" ~ "Likely, with assumptions",
             reproducible == "No" ~ "No"
           ),
         all6_factor = 
           case_when(
             all6 == 6 ~ "Yes",
             TRUE ~ "No"
           ),
         alpha_factor = 
           case_when(
             alpha_text == 0.05 ~ "0.05",
             is.na(alpha_text) ~ "Not reported",
             TRUE ~ "Other"
           ),
         power_factor = 
           case_when(
             power_text == 0.8 ~ "0.80",
             power_text == 0.95 ~ "0.95",
             is.na(power_text) ~ "Not reported",
             TRUE ~ "Other"
           ),
         effect_size_type_factor = 
           case_when(
             effect_size_type == "d" ~ "d",
             effect_size_type == "f" ~ "f",
             effect_size_type == "Other: Non-standardized (e.g., 5 points on a questionnaire scale)." ~ "Non-standardized",
             effect_size_type == "No" ~ "Not reported",
             TRUE ~ "Other"
           ),
         effect_size_value_factor = 
           case_when(
             effect_size_value == 1 ~ "Reported",
             effect_size_value == 0 ~ "Not reported"
           ),  
         stat_test_factor = 
           case_when(
             stat_test == "ANOVA" ~ "ANOVA",
             stat_test == "t-test" ~ "t-test",
             stat_test == "the statistical test is NOT reported" ~ "Not reported",
             TRUE ~ "Other"
           ),
         sample_size_factor =
             case_when(
             sample_size == 1 ~ "Reported",
             sample_size == 0 ~ "Not reported"
             ),
         mult_compare_factor = 
           case_when(
             grepl("contains multiple analyses", mult_compare) ~ "No, and multiple analyses are performed",
             grepl("no reason to account for multiple comparisons", mult_compare) ~ "No, but a single outcome was identified",
             mult_compare == "Unsure" ~ "Unsure",
             mult_compare == "Yes" ~ "Yes"
           ),
         error_factor = 
           case_when(
             grepl("Likely", error) ~ "Unsure",
             grepl("Unsure", error) ~ "Unsure",
             error == "No" ~ "No",
             error == "Yes" ~ "Yes" 
           ),
         match_factor = 
           case_when(
             grepl("Unsure", match) ~ "Unsure",
             match == "No" ~ "No",
             match == "Yes" ~ "Yes",
             grepl("protocol", match) ~ "NA (protocol)"
           )
  )

# Order the levels of the factors for clarity
df1$reproducible_factor <- factor(df1$reproducible_factor, levels = c("Yes, without assumptions", "Likely, with assumptions", "No"))
df1$all6_factor <- factor(df1$all6_factor, levels = c("Yes", "No"))
df1$alpha_factor <- factor(df1$alpha_factor, levels = c("0.05", "Other", "Not reported"))
df1$power_factor <- factor(df1$power_factor, levels = c("0.80", "0.95", "Other", "Not reported"))
df1$effect_size_type_factor <- factor(df1$effect_size_type_factor, levels = c("d", "f", "Non-standardized", "Other", "Not reported"))
df1$effect_size_value_factor <- factor(df1$effect_size_value_factor, levels = c("Reported", "Not reported"))
df1$stat_test_factor <- factor(df1$stat_test_factor, levels = c("ANOVA", "t-test", "Other", "Not reported"))
df1$sample_size_factor <- factor(df1$sample_size_factor, levels = c("Reported", "Not reported"))
df1$mult_compare_factor <- factor(df1$mult_compare_factor, levels = c("Yes", "No, and multiple analyses are performed", "No, but a single outcome was identified", "Unsure"))
df1$error_factor <- factor(df1$error_factor, levels = c("Yes", "No", "Unsure"))
df1$match_factor <- factor(df1$match_factor, levels = c("Yes", "No", "Unsure", "NA (protocol)"))


# Create dataframe with the information to present in a table
t_repro <- bind_rows(table(df1$reproducible_factor) %>% as.data.frame(),
              table(df1$all6_factor) %>% as.data.frame(),
              table(df1$alpha_factor) %>% as.data.frame(),
              table(df1$power_factor) %>% as.data.frame(),
              table(df1$effect_size_type_factor) %>% as.data.frame(),
              table(df1$effect_size_value_factor) %>% as.data.frame(),
              table(df1$stat_test_factor) %>% as.data.frame(),
              table(df1$sample_size_factor) %>% as.data.frame()
) %>% 
  # add columns with the percentage and confidence intevals
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, denom))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, denom))) %>% 
  ungroup()

colnames(t_repro) <- c("Transparency element",
                       paste0("N=", denom),
                       "Percent",
                       "(95% CI)"
                       )

median_sample_size <- median(df1$sample_size_text, na.rm = TRUE)
IQR_low_sample_size <- quantile(df1$sample_size_text, 0.25, na.rm = TRUE) 
IQR_high_sample_size <- quantile(df1$sample_size_text, 0.75, na.rm = TRUE) %>% round(0)

```

```{r characteristics}
# Create a table of the publishers to add to the characteristics table
publishers_main <- table(df1$publisher) %>% as.data.frame() %>% arrange(desc(Freq))

# Sum rows 8 to 38 for publishers with only 1 or 2 articles in our sample
publishers_other <- sum(publishers_main$Freq[8:38])
publishers_other <- data.frame(Var1 = "Other", Freq = publishers_other)

# Combine the first 7 rows and the 'other' row into a new dataframe
publishers <- rbind(publishers_main[1:7, ], publishers_other)

# I rewrote these, otherwise they are all in capital letter and it doesn't look nice in the table.
publishers$Var1 <- c(
                    "BMC",
                    "MDPI",
                    "Frontiers Media SA",
                    "PLOS",
                    "Springer",
                    "Wiley",
                    "Nature Portfolio",
                    "Other"
                    )

# Create a table with the article characteristics (in other words, the typical "Table 1" of many manuscripts)
t_char <- bind_rows(
              table(df1$participants) %>% as.data.frame(),
              table(df1$pub_year) %>% as.data.frame(),
              table(df1$protocol) %>% as.data.frame() %>% arrange(Freq),
              table(df1$multiple) %>% as.data.frame() %>% arrange(Freq),
              publishers
              ) %>% 
  mutate(perc = paste0((Freq / denom) %>% round(2) * 100, "%")
  )

colnames(t_char) <- c("Article characteristic",
                       paste0("N=", denom),
                       "Percent"
                       )

median_impact_factor <- median(df1$impact_factor, na.rm = TRUE) %>% round(1)
min_impact_factor <- min(df1$impact_factor, na.rm = TRUE) %>% round(1)
max_impact_factor <- max(df1$impact_factor, na.rm = TRUE) %>% round(1)
n_no_impact_factor <- sum(is.na(df1$impact_factor))

pub <- table(df1$publisher) %>%
  as.data.frame() %>% 
  arrange(desc(Freq))

colnames(pub) <- c("publisher", "Freq")

```

```{r quality}
# Make a dataframe of the justification, so they are all binary, ordered from most common to least common
justification <- data.frame(
  Var1 = c(
          "Previously published research",
          "Effect size conventions",
          "General reference to another study",
          "Pilot data",
          "Effect size of interest",
          "Other",
          "No justification reported"
      ),
  Freq = c(
          sum(grepl("Previously published research", df1$justification)),
          sum(grepl("Conventions", df1$justification)),
          sum(grepl("Reference to other study to justify their calculation in general", df1$justification)),
          sum(grepl("pilot data", df1$justification)),
          sum(grepl("MCID", df1$justification)),
          sum(grepl("Other", df1$justification)),
          sum(grepl("No justification", df1$justification))
        )
)

# Combine all quality factors into a single dataframe
t_qual <- bind_rows(table(df1$match_factor) %>% as.data.frame(),
                    table(df1$error_factor) %>% as.data.frame(),
                    table(df1$mult_compare_factor) %>% as.data.frame(),
                    justification
                    ) %>% 
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, denom))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, denom))) %>% 
  ungroup()

# Replace the ci for the one element with 0 hits with NA
i <- grepl("- <1%", t_qual$ci) %>% 
  which()
t_qual$ci[i] <- "(NA)"

colnames(t_qual) <- c("Quality measure",
                       paste0("N=", denom),
                       "Percent",
                       "(95% CI)"
                       )

```

```{r kappa}
inter_rater <- function(df, variables, prescreen_questions){
  
  # Initialize an empty list to store results
    kappa_results <- data.frame(matrix(nrow = length(variables), ncol = 5))
    colnames(kappa_results) <- c("k", "percent_agreed", "n_disagreed", "n_total", "n_categories")
    rownames(kappa_results) <- variables
    
  # Loop through each variable
  for (var in variables) {
    
    if(prescreen_questions == TRUE){
    # Subset data for each coder and specific variable
    data1 <- subset(df, coder == "RTT")[, var]
    data2 <- subset(df, coder == "EZ")[, var]
    } else {
    data1 <- subset(df, coder_number == 1)[, var]
    data2 <- subset(df, coder_number == 2)[, var]
    }
      
    # Combine into a matrix
    combined_data <- matrix(c(data1, data2), ncol = 2)
    
    # Calculate Cohen's Kappa
    result <- kappa2(combined_data)
    
    # Store the result in the list
    kappa_results[[var, "k"]] <- result$value %>% round(2)
    kappa_results[[var, "percent_agreed"]] <- (sum(data1 == data2, na.rm = TRUE) / result$subjects) %>% round(2)
    kappa_results[[var, "n_disagreed"]] <- sum(data1 != data2, na.rm = TRUE)
    kappa_results[[var, "n_total"]] <- result$subjects
    kappa_results[[var, "n_categories"]] <- n_distinct(df[[var]])
  }
  return(kappa_results)
}


# Select variables to calculate inter-rater agreement for when looking at all articles we assessed (both included and excluded articles). The variables it doesn't make sense to calculate agreement for are commented out.
variables1 <- c(
  "id",
#  "coder",
#  "coder_text",
#  "coder_numer",
  "pmcid",
  "protocol",
  "include"
)

# Call the function we create above
kappa_include <- inter_rater(df, variables1, TRUE)

# Identify ids for included articles
id_include <- df %>% filter(grepl("/", coder)) %>% 
  filter(include == "Include") %>% 
  select(id)

# Remove excluded articles based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df_kappa <- df %>% filter(id %in% id_include$id)

# Now that I've removed the excluded articles, I can calculate inter-rater agreement for the variables coded for all included article. All these questions were answered before the survey allowed the coder to select the option "I don't have the expertise for this article".

# Select variables to calculate inter-rater agreement for when looking at all included articles. The variables it doesn't make sense to calculate agreement for are commented out.
variables2 <- c(
  "participants",
#  "participants_text",
  "journal", # small difference in typing may result in different coding
#  "publisher", # recorded by one coder after all other coding was done (thus cannot calculate IRR)
  "pub_year",
  "impact_factor",
#  "verbatim",
  "power_calc_type",
# "power_calc_type_text",
  "multiple"
#  "stats_knowledge",
)

# Recode NAs to -100 in the "impact_factor" column
df_kappa$impact_factor <- ifelse(is.na(df_kappa$impact_factor), -100, df_kappa$impact_factor)

kappa_any_power_calc <- inter_rater(df_kappa, variables2, TRUE)

# Return the impact factors to NA. I only needed them to be a number for the n_disagreement variable

# Identify ids for articles not solving for sample size
id_sample_size_calc <- df %>% filter(grepl("/", coder)) %>% 
  filter(grepl("Solves for sample size \\(often called a priori\\)", power_calc_type)) %>% # Note, I have to use grepl() here, because there are two instances where the power_calc_type column contains more than one type of power calculation. And thus, a string match with "==" does not catch these two.
  filter(!id %in% c(143, 146)) %>%
  select(id)

# Remove articles that don't solve for sample size based on their id. This is necessary to calculate inter-rater agreement for the remaining variables. 
df_kappa <- df_kappa %>% filter(id %in% id_sample_size_calc$id)

# Calculate inter-rater agreement for all relevant variables

# Select variables to calculate inter-rater agreement for when looking only at articles with a sample size calculation. The variables it doesn't make sense to calculate agreement for are commented out.
variables3 <- c(
  "version",
  "version_text",
  "power",
  "power_text",
  "alpha",
  "alpha_text",
  "sample_size",
  "sample_size_text",
  "effect_size_type",
#  "effect_size_type_other_standardized_text",
#  "effect_size_type_other_nonstandardized_text",
  "effect_size_value",
  "effect_size_value_text",
  "stat_test",
#  "stat_test_other_regression_text",
#  "stat_test_other_nonregression_text",
#  "other_info_missing",
  "reproducible",
#  "reproducible_text",
  "justification",
  "just_previous",
  "just_pilot",
  "just_convention",
  "just_mcid",
  "just_none",
  "just_ref",
  "just_other",
# "justification_text",
  "mult_compare",
#  "mult_compare_text",
  "anova_within_between",
  "match",
#  "match_text",
  "error"
#  "error_text",
#  "impact",
#  "comments_calc",
#  "comments_general",
#  "posthoc_resolving_notes"
)

kappa_sample_size_calc <- inter_rater(df_kappa, variables3, FALSE)

# Merge all inter-rater agreement scores
kappa_all <- rbind(kappa_include,
                   kappa_any_power_calc,
                   kappa_sample_size_calc
)

kappa_all <- kappa_all %>% 
  mutate(percent_agreed = paste0(percent_agreed * 100, "%"))

```

```{r anova}

filename <- paste0(getwd(), "/../data/anova_raw.csv")
raw_anova <- read.csv(filename, fileEncoding = "UTF-8", header = T)
df_anova_new <- raw_anova %>% 
  select(-c(coder_text, participants_text)
         )

df1_anova_new <- df_anova_new %>% filter(grepl("/", coder))

# Format the anova data from the main dataset to match the number of columns in the anova specific dataset
df1_anova_original <- df1 %>% select(c(id,	
                                       coder,	
                                       pmcid,
                                       protocol,	
                                       participants,	
                                       journal,	pub_year,	
                                       impact_factor,	
                                       verbatim,	
                                       reproducible,	
                                       reproducible_text,	
                                       anova_within_between,	
                                       comments_calc,	
                                       comments_general
                                       )
) %>% 
  filter(grepl("Yes, ", anova_within_between)
  )

# This line is needed so the two datasets will bind properly
df1_anova_new$pub_year <- as.character(df1_anova_new$pub_year)

# Combine the rows that had anovas in the main dataset with the anova specific dataset
df1_anova <- bind_rows(df1_anova_original, df1_anova_new)

anova_results <- data.frame(
  item = c(
    "Non-default option",
    "Default option",
    "Unsure"
  ),
  Freq = c(
    sum(grepl("Yes, and the researchers selected a non-default option.", df1_anova$anova_within_between)),
    sum(grepl("Yes, but the researchers use the default option.", df1_anova$anova_within_between)),
    sum(grepl("Yes, but I cannot reasonably assume which option they used.", df1_anova$anova_within_between))
  )
) %>% 
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, nrow(df1_anova)))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, nrow(df1_anova)))) %>% 
  ungroup()

```

```{r effect_size_height}

cohensd <- function(m1, m2, sem1, sem2, n1, n2){ 
  # Calculate standard deviations from SEM
  sd1 <- sem1 * sqrt(n1)
  sd2 <- sem2 * sqrt(n2)
  
  # Calculate pooled standard deviation
  sd_pooled <- sqrt((sd1^2 + sd2^2) / 2)
  
  dif <- m1 - m2
  
  # Calculate Cohen's d
  cohensd <- (m1 - m2) / sd_pooled
  
  all <- c(
    sd1,
    sd2,
    sd_pooled,
    dif,
    cohensd
  )
  
  return(all)
}

# Data taken from Table 9 and Table 11 of https://www.cdc.gov/nchs/data/series/sr_03/sr03-046-508.pdf

# All people over 20 years of age
es_height <- cohensd(
  m1 = 175.3, 
  m2 = 161.3,
  sem1 = 0.19,
  sem2 = 0.19,
  n1 = 5092,
  n2 = 5510
)

# All people from 20-29 years of age
es_height_age <- cohensd(
  m1 = 175.8,
  m2 = 162.6,
  sem1 = 0.30,
  sem2 = 0.32,
  n1 = 802,
  n2 = 892
)

# Non-Hispanic white people from 20-39 years of age
es_height_age_race <- cohensd(
  m1 = 178.0,
  m2 = 164.5,
  sem1 = 0.39,
  sem2 = 0.31,
  n1 = 490,
  n2 = 545
)

```

```{r by_participant}

# Filter for articles with reproducible or likely reproducible sample size calculations
df1_repro <- df1 %>% 
  filter(grepl("Yes", reproducible)) 

# Sum of participants in studies with reproducible sample size calculations
n_participants_repro <- sum(df1_repro$sample_size_text)

# Filter for articles with non-reproducible sample size calculations
df1_non_repro <- df1 %>% 
  filter(grepl("No", reproducible)) %>% 
  filter(!is.na(sample_size_text))

# Sum of participants in studies with non-reproducible sample size calculations
n_participants_non_repro <- sum(df1_non_repro$sample_size_text) +
                              5 * median(df1_non_repro$sample_size_text) # to account for the 5 sample sizes that were not reported for the non-reproducible sample size calculations.

perc_participants_repro <- (n_participants_repro / (n_participants_repro + n_participants_non_repro)) %>% round(2)


```

<!-- OUTPUTS -->

```{r, table_calc_type, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(calc_type, 
             caption = "Table 1. Types of power calculations.", 
             booktabs = T, 
             linesep = "\\addlinespace",
             align = "lrrr") %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote("The sum of the first column is greater than n = 141 because four articles performed multiple power calculations that solved for different variables. We only coded `Sample size (after completing the study)' when an article unambiguously performed their calculation for a future study. Some articles used G*Power to calculate effect sizes after a study was complete.", notation = "none", threeparttable = T)
```

```{r, table_reproducibility, include = TRUE, echo = FALSE, results = "asis"}
# make marker numbers for where to break the table using kable
r1 <- nlevels(df1$reproducible_factor)
r2 <- r1 + nlevels(df1$all6_factor)
r3 <- r2 + nlevels(df1$alpha_factor)
r4 <- r3 + nlevels(df1$power_factor)
r5 <- r4 + nlevels(df1$effect_size_type_factor)
r6 <- r5 + nlevels(df1$effect_size_value_factor)
r7 <- r6 + nlevels(df1$stat_test_factor)
r8 <- r7 + nlevels(df1$sample_size_factor)

knitr::kable(t_repro, 
             caption = "Table 2. Reproducibility of sample size calculations performed using G*Power.", 
             booktabs = T, 
             linesep = "\\addlinespace", 
             align = "lrrr") %>%
kable_styling() %>%
  pack_rows("Reproducible*", 1, r1) %>%
  pack_rows("All 6 elements reported†", r1+1, r2) %>%
  pack_rows("Alpha", r2+1, r3) %>%
  pack_rows("Power", r3+1, r4) %>%
  pack_rows("Effect size type", r4+1, r5) %>%
  pack_rows("Effect size value", r5+1, r6) %>%
  pack_rows("Statistical test", r6+1, r7) %>%
  pack_rows("Sample size‡", r7+1, r8) %>%
  kable_styling(latex_options = "striped") %>% 
  add_footnote(paste0("*We use the term `likely' because we cannot be certain that all of our assumptions were correct. †These six elements are: alpha, power or beta, effect size type, effect size value, statistical test, and sample size. We considered statistical test reported if they named the general test, even if details were missing (e.g., reporting an ANOVA, but not what type of ANOVA). ‡The median sample size was ", median_sample_size, " (IQR: ", IQR_low_sample_size, " to ", IQR_high_sample_size, ")."), notation = "none", threeparttable = T)
```

```{r, table_characteristics, include = TRUE, echo = FALSE, results = "asis"}
r1 <- nrow(table(df1$participants))
r2 <- r1 + nrow(table(df1$pub_year))
r3 <- r2 + nrow(table(df1$protocol))
r4 <- r3 + nrow(table(df1$multiple))
r5 <- r4 + nrow(publishers)

knitr::kable(t_char, 
             caption = "Table 3. Article characteristics", 
             booktabs = T, 
             linesep = "\\addlinespace", 
             align = "lrr") %>%
kable_styling() %>%
  pack_rows("Unit of study", 1, r1) %>%
  pack_rows("Year of publication", r1+1, r2) %>%
  pack_rows("Protocol article", r2+1, r3) %>%
  pack_rows("Multiple sample size calculations", r3+1, r4) %>%
  pack_rows("Publisher", r4+1, r5) %>%
  kable_styling(latex_options = "striped") %>% 
  add_footnote(paste0("The median journal impact factor was ", median_impact_factor, " (range: ", min_impact_factor, " to ", max_impact_factor, "). ", n_no_impact_factor, " articles were pubished in journals that did not have an impact factor. Year 2023 contains fewer articles than the preceeding years because we only sampled until May 31, rather than the entire year."), notation = "none", threeparttable = T)
```

```{r, table_qual, include = TRUE, echo = FALSE, results = "asis"}
# Index for kable function
r1 <- nlevels(df1$match_factor)
r2 <- r1 + nlevels(df1$error_factor)
r3 <- r2 + nlevels(df1$mult_compare_factor)
r4 <- r3 + nrow(justification)

knitr::kable(t_qual, 
             caption = "Table 4. Quality of the sample size calculations performed using G*Power", 
             booktabs = T, 
             linesep = "\\addlinespace",
             align = "lrrr") %>%
kable_styling() %>%
  pack_rows("Analysis match in results section", 1, r1) %>%
  pack_rows("Error", r1+1, r2) %>%
  pack_rows("Adjusted for multiple comparisons", r2+1, r3) %>%
  pack_rows("Justification for chosen effect size*", r3+1, r4) %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote("*Some articles provided more than one justification for their chosen sample size, and thus the sum of the percentages is greater than 100%.", notation = "none", threeparttable = T)
```

```{r, table_anova, include = TRUE, echo = FALSE, results = "asis"}
  knitr::kable(anova_results, 
               caption = "Table 5. Selection of default ANOVA option", 
               booktabs = T, 
               linesep = "\\addlinespace",
               col.names = c(
                 "Option used",
                 "N=36",
                 "Percent",
                 "(95% CI)"
               )
  ) %>%
  kable_styling(latex_options = "striped")
```

```{r, table_n_articles, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(cis_out, 
             caption = "Supplementary Table 1. Estimated number of articles published between 1 Jan 2017 and 31 May 2022 that reference G*Power", 
             booktabs = T, 
             linesep = "\\addlinespace",
             align = "rr") %>%
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  kable_styling(font_size = 10) %>% 
  add_footnote(paste0("We excluded articles that discuss G*Power, but do not report using this software for a power calculation. The table includes rows for publications that report using G*Power for any power calculation related to any statistical test (any power calculation), a power calculation for any statistical test that solves for sample size (sample size calculation), and a power calculation for an ANOVA that solves for sample size (ANOVA sample size calculation). The total number of articles in each database from 1 Jan 2017 to 31 May 2022 is: PubMed Central ", prettyNum(pmc_total, big.mark=",", scientific=F), "; PubMed ", prettyNum(pubmed_total, big.mark=",", scientific=F), ". Numbers are rounded to the nearest thousand to avoid suggesting a higher level of precision than our method of estimation can provide."), notation = "none", threeparttable = T)
```

```{r, table_kappa, include = TRUE, echo = FALSE, results = "asis"}
  knitr::kable(kappa_all, 
               caption = "Supplementary Table 2. Inter-rater agreement", 
               booktabs = T, 
               linesep = "\\addlinespace",
               align = "lrrrrr",
               col.names = c("Variable",
                         "Cohen's k",
                         "Percent agreed",
                         "n disagreed",
                         "n total",
                         "n categories"
                         )) %>%
    kable_styling(latex_options = "striped") %>%
    add_footnote("The variables are listed as they appear in the open data. See the data dictionary for a description of each variable. Cohen's kappa is mostly irrelevant for variables with a large number of categories, and can be ignored. Not all variables were relevant for all articles we coded; thus, 'n total' differs among the variables. `justification' was coded as a multiple selection question with 7 options. We re-coded this variable into 7 binary variables and calculate the inter-rater agreement for each one.", notation = "none", threeparttable = T)
```


Percentage of PMC articles that use GPower for any power calculation, with 95% CIs: `r cis_prop_any`  
Percentage of PMC articles that use GPower for a sample size calculation, with 95% CIs: `r cis_prop_sample_size`   
n articles in PMC per 1 that uses GPower for any calculation: `r round(n_per_any, 0)`  
n articles in PMC per 1 that uses GPower for a sample size calculation: `r round(n_per_sample_size, 0)`  

Participants that partook in studies with reproducible or likely reproducible sample size calculations: n = `r n_participants_repro`  
Participants that partook in studies with non-reproducible sample size calculations: approximately n = `r n_participants_non_repro` (we assumed the sample size for 5 of these articles that did not report sample size)  
Total number of participants: `r n_participants_repro + n_participants_non_repro`  
Proportion of participants that partook in a study with a reproducible or likely reproducible sample size calculation: `r perc_participants_repro`  

Cohen's *d* for height difference between men and women in the US (based on NHANES data Tables 9 and 11: https://www.cdc.gov/nchs/data/series/sr_03/sr03-046-508.pdf):  
All people aged 20 and over: *d* = `r es_height[5] %>% round(2)`  
All people aged 20-29: *d* = `r es_height_age[5] %>% round(2)`  
Non-Hispanic, white aged 20-39: *d* = `r es_height_age_race[5] %>% round(2)`  

```{r table_csvs}
write_csv(calc_type, paste0(getwd(), "/../results/table1.csv"))
write_csv(t_repro, paste0(getwd(), "/../results/table2.csv"))
write_csv(t_char, paste0(getwd(), "/../results/table3.csv"))
write_csv(t_qual, paste0(getwd(), "/../results/table4.csv"))
write_csv(anova_results, paste0(getwd(), "/../results/table5.csv"))
write_csv(cis_out, paste0(getwd(), "/../results/supplementary_table1.csv"))
write_csv(kappa_all, paste0(getwd(), "/../results/supplementary_table2.csv"))
```


