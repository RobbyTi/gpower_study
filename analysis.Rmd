---
title: "gpower_analysis_230915"
author: "Robert Thibault"
date: "05/10/2022"
output: 
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE) # this option stops the code chunks from being output from the knit
set.seed(1313)
```

```{r packages}
library(tidyverse) # for cleaner code
library(knitr) # for kable function
library(kableExtra) # for kable table styling
library(irr) # for inter-rater agreement calculations
library(english) # to convert numbers to word equivalents via the function: words() 
library(statpsych) # for the precision analysis
library(table1) # for making Table 1
```

```{r clean}
# load the raw data
filepath <- paste0(getwd(), "/data/data_main_raw.csv" )
raw <- read.csv(filepath, header = TRUE)
df <- raw

df <- df %>% 
  # remove the columns for the initials of coders other than RTT, EZ, and HP (in the end, we only had these 3 coders)
  select(-coder_text) %>% 
  # filter out the rows labeled "resolveNotes", in which we took some notes when resolving differences in coding in Google sheets.
  filter(coder != "resolveNotes") %>% 
  filter(id != "") %>%   # remove two irrelevant rows that Qualtrics exports
  filter(id != "156") %>%  # remove extra paper RTT coded after we had reached our sample size, but that no one else coded.
  
  ############# TEMPORARILY REMOVE THESE TWO, UNTIL HUGO CODES THEM
    filter(id != "143") %>%  
    filter(id != "146") %>%  
  ################
  
  # recode the $power_text column to 0.80, where relevant. This is necessary as we updated the Qualtrics extraction form to add a button for "Yes (0.80)" to speed up coding
  mutate(
    power_text = if_else(power == "Yes (0.80)", "0.80", power_text),
    power = if_else(power == "Yes (0.80)", "Yes", power),
    # repeat for $alpha column
    alpha_text = if_else(alpha == "Yes (0.05)", "0.05", alpha_text),
    alpha = if_else(alpha == "Yes (0.05)", "Yes", alpha)
  )

# recode Yes/No columns to 1 (yes), 0 (no), and NA
df <- 
  df %>% 
  mutate_at(c("version", "power", "alpha", "sample_size", "effect_size_value"),
      ~ case_when(
      . == "Yes" ~ 1L,
      . == "No" ~ 0L,
      is.na(.) ~ NA_integer_) 
      ) %>% 
  mutate(effect_size_type_binary = 
      case_when(
      effect_size_type == "No" ~ 0L,
      effect_size_type == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  stat_test_binary = 
      case_when(
      stat_test == "the statistical test is NOT reported" ~ 0L,
      stat_test == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  # make new column that checks how many sample size calculationss include all 6 elements
  all6 = (
    power + alpha + sample_size + effect_size_type_binary + effect_size_value + stat_test_binary
    ),
  # recode some columns to binary that weren't previously only "Yes" and "No"
  all6_binary = 
      case_when(
      all6 == 6 ~ 1L,
      all6 < 6 ~ 0L,
      is.na(all6) ~ NA_integer_
      ),
  reproducibile_binary = 
      case_when(
      grepl("Yes", reproducible) ~ 1L,
      reproducible == "No" ~ 0L,
      is.na(reproducible) ~ NA_integer_
      ),
  justification_binary = 
      case_when(
      grepl("No justification", justification) ~ 0L,
      is.na(justification) ~ NA_integer_,
      TRUE ~ 1L,
      )
  )

# recode a few variables we changed during coding
df <- df %>% 
  mutate(anova_within_between = 
      case_when(
      grepl("the default option without accounting", anova_within_between) ~ "Yes, but the researchers use the default option.",
      grepl("Yes, and", anova_within_between) ~ "Yes, and the researchers selected a non-default option.",
      TRUE ~ as.character(anova_within_between)  # Keep all other values as they are
      )
  ) %>% 
  mutate(error = 
      case_when(
      grepl("Likely", error) ~ "Unsure (there's not enough information to reasonably code yes or no).",
      TRUE ~ as.character(error)  # Keep all other values as they are
      )
  ) %>% 
  mutate(participants = 
      case_when(
      grepl("Humans", participants) ~ "Human",
      TRUE ~ "Non-human animal"
      )
  )


# recode a few columns to numerics
df$power_text <- as.numeric(df$power_text)
df$alpha_text <- as.numeric(df$alpha_text)
df$sample_size_text <- as.numeric(df$sample_size_text)
df$effect_size_value_text <- as.numeric(df$effect_size_value_text)
df$impact_factor <- as.numeric(df$impact_factor)

# decompose the justifications variable
df$just_previous <- ifelse(grepl("Previous", df$justification), 1, 0)
df$just_pilot <- ifelse(grepl("pilot data", df$justification), 1, 0)
df$just_convention <- ifelse(grepl("Convention", df$justification), 1, 0)
df$just_mcid <- ifelse(grepl("MCID", df$justification), 1, 0)
df$just_none <- ifelse(grepl("No justification", df$justification), 1, 0)
df$just_ref <- ifelse(grepl("Reference to other study", df$justification), 1, 0)
df$just_other <- ifelse(grepl("Other", df$justification), 1, 0)

# make a new dataframe that only includes the resolved coding
df1_all <- df %>% filter(grepl("/", coder))

# filter for articles that use GPower to solve for sample size
df1 <- df1_all %>% filter(grepl("Solves for sample size", power_calc_type))

# remove blank rows
df1 <- df1 %>% 
  select(-c(coder_number, participants_text, power_calc_type_text))

filename <- paste0(getwd(), "/data/data_main_clean.csv")
write_csv(df1, filename)
```

```{r functions}

# create function to calculate the point estimates and confidence intervals based on Monte Carlo sampling
ci_calc <- function(num, denom){ 
     sim <- rbeta(100000, shape1=num, shape2=(denom-num)) # simulate the distribution
     point <- (quantile(sim, probs=0.5)) # calculate the point estimate and 95% CI bounds
     lb <- (quantile(sim, probs=0.025))
     ub <- (quantile(sim, probs=0.975))
     cis <- c(point, lb, ub)
  return(cis)
}

#create function to create point estimates and confidence intervals as they'll appear in the table output
ci_clean <- function(ci){
  ci <- ci %>% round(2)
  ci_out <- paste0(ci[1], " (", ci[2], " - ", ci[3], ")")
  return(ci_out)
}

ci_percent <- function(ci){
  ci <- ci %>% round(2) * 100
  if(ci[2] ==0){ci[2] <- "<1"}
  if(ci[3] ==0){ci[3] <- "<1"}
  ci_out <- paste0("(", ci[2], " - ", ci[3], "%)")
  return(ci_out)
}

point_percent <- function(ci){
  ci <- ci %>% round(2) * 100
  ci_out <- paste0(ci[1], "%") %>% 
  return(ci_out)
}


```

```{r n_articles}
# this chunk estimates the total number of published articles that use GPower for doing (1) any power calculation, (2) an a priori sample size calculation, or (3) an a priori sample size calculation for an ANOVA

denom_all <- nrow(df1_all) # number of articles coded
denom <- nrow(df1) # number or articles with a sample size calc
n_included <- sum(df1_all$include == "Include")  # number of articles with any power calc
n_anova <- sum(grepl("Yes", df1$anova_within_between)) # number of articles with a power calc that solves for sample size for a relevant ANOVA
pmc_total = 3285893 # total number of PMC articles published in the time of our query: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pmc_hits = 22188  # number of search hits from our query (GPower OR “G Power”) AND ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pubmed_total = 7318980 # total number of PubMed articles published in that period: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [Date - Entry] : "2022/05/31" [Date - Entry])
proportion <- pmc_hits/pmc_total # proportion of PMC articles that were hits
mult_fact <- pubmed_total/pmc_total # the factor to multiply our findings for PMC articles to get an estimate for all PubMed articles.

#initialize dataframe for point estimates and confidence intervals
cis <- data.frame(matrix(nrow=6, ncol=1))

# run the function to calculate point estimates and confidence intervals 
cis <- as.data.frame(
  bind_cols(ci_calc(n_included, denom_all),
            ci_calc(denom, denom_all),
            ci_calc(n_anova, denom_all)
           )
)

# calculate cis for PMC database
cis_pmc <- (cis * pmc_total * proportion) %>% 
  round(-3) %>% 
  summarise_all(ci_clean) %>% 
  t() %>% 
  as.data.frame()

# calculate cis for PubMed database
cis_pubmed <- (cis * pubmed_total * proportion) %>% 
  round(-3) %>% 
  summarise_all(ci_clean) %>% 
  t() %>% 
  as.data.frame()


cis_out <- bind_cols(cis_pmc, cis_pubmed)

rownames(cis_out) <- c("Any power calculation",
                       "Sample size calculation",
                       "ANOVA sample size calculation"
                       )

colnames(cis_out) <- c("PubMed Central (95% CI)",
                       "PubMed (95% CI)"
                       )

```

```{r, table_n_articles, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(cis_out, 
             caption = "Table 1. Estimated number of articles published between 1 Jan 2017 and 31 May 2022 that reference the G*Power software.", 
             booktabs = T, 
             linesep = "\\addlinespace",
             align = "rr") %>%
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  kable_styling(font_size = 10) %>% 
  add_footnote(paste0("We excluded articles that discuss G*Power, but do not report using this software for a power calculation. The table includes rows for publications that report using G*Power for any power calculation related to any statistical test (Any power calculation), a power calculation for any statistical test that solves for sample size (Sample size calculation), and a power calculation for an ANOVA that solves for sample size (ANOVA sample size calculation). The total number of articles in each database from 1 Jan 2017 to 31 May 2022 is: PubMed Central ", prettyNum(pmc_total, big.mark=",", scientific=F), "; PubMed ", prettyNum(pubmed_total, big.mark=",", scientific=F), "Numbers are rounded to the nearest thousand to avoid suggesting a higher level of precision than this estimation provides."), notation = "none", threeparttable = T)
```

```{r calc_type}


# calculate how many articles report solving for each of these elements
calc_type <- data.frame(
  type = c("Sample  size",
           "Power",
           "Effect size",
           "Sample size (after completing the study)",
           "Unsure"
  ),
  Freq = c(sum(grepl("sample size", df1_all$power_calc_type)),
           sum(grepl("power", df1_all$power_calc_type)),
           sum(grepl("effect size", df1_all$power_calc_type)),
           sum(grepl("Other", df1_all$power_calc_type)),
           sum(grepl("Unsure", df1_all$power_calc_type))
  )
) %>% 
# calculate percentage and confidence intervals
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, n_included))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, n_included))) %>% 
  ungroup()

colnames(calc_type) <- c("Power calculation solving for",
                         paste0("N=", n_included),
                         "Percentage", 
                         "(95% CI)"
)

```

```{r, table_calc_type, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(calc_type, 
             caption = "Table 2. Types of power calculations.", 
             booktabs = T, 
             linesep = "\\addlinespace",
             align = "lrrr") %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote("The sum of the first column is greater than n = 140 because four articles performed multiple power calculations that solved for different elements.We only coded *Sample size (after completing the study)* when an article unambiguously described a calculation for a future study. ", notation = "none", threeparttable = T)
```

```{r reproducibility}
# recode columns to have fewer factors that will fit more cleanly into a table
df1 <- df1 %>% 
  mutate(reproducible_factor = 
           case_when(
             reproducible == "Yes, based solely on the information in the article or its supplementary material" ~ "Yes, without assumptions",
             reproducible == "Yes, but I've had to make some assumptions. (please list the assumptions you made)" ~ "Likely, with assumptions",
             reproducible == "No" ~ "No"
           ),
         all6_factor = 
           case_when(
             all6 == 6 ~ "Yes",
             TRUE ~ "No"
           ),
         alpha_factor = 
           case_when(
             alpha_text == 0.05 ~ "0.05",
             is.na(alpha_text) ~ "Not reported",
             TRUE ~ "Other"
           ),
         power_factor = 
           case_when(
             power_text == 0.8 ~ "0.80",
             power_text == 0.95 ~ "0.95",
             is.na(power_text) ~ "Not reported",
             TRUE ~ "Other"
           ),
         effect_size_type_factor = 
           case_when(
             effect_size_type == "d" ~ "d",
             effect_size_type == "f" ~ "f",
             effect_size_type == "Other: Non-standardized (e.g., 5 points on a questionnaire scale)." ~ "Non-standardized",
             effect_size_type == "No" ~ "Not reported",
             TRUE ~ "Other"
           ),
         effect_size_value_factor = 
           case_when(
             effect_size_value == 1 ~ "Reported",
             effect_size_value == 0 ~ "Not reported"
           ),  
         stat_test_factor = 
           case_when(
             stat_test == "ANOVA" ~ "ANOVA",
             stat_test == "t-test" ~ "t-test",
             stat_test == "the statistical test is NOT reported" ~ "Not reported",
             TRUE ~ "Other"
           ),
         sample_size_factor =
             case_when(
             sample_size == 1 ~ "Reported",
             sample_size == 0 ~ "Not reported"
             ),
         mult_compare_factor = 
           case_when(
             grepl("contains multiple analyses", mult_compare) ~ "No, and multiple analyses are performed",
             grepl("no reason to account for multiple comparisons", mult_compare) ~ "No, but a single outcome was identified",
             mult_compare == "Unsure" ~ "Unsure",
             mult_compare == "Yes" ~ "Yes"
           ),
         error_factor = 
           case_when(
             grepl("Likely", error) ~ "Unsure",
             grepl("Unsure", error) ~ "Unsure",
             error == "No" ~ "No",
             error == "Yes" ~ "Yes" 
           ),
         match_factor = 
           case_when(
             grepl("Unsure", match) ~ "Unsure",
             match == "No" ~ "No",
             match == "Yes" ~ "Yes",
             grepl("protocol", match) ~ "NA (protocol)"
           )
  )

# order the levels of the factors for clarity
df1$reproducible_factor <- factor(df1$reproducible_factor, levels = c("Yes, without assumptions", "Likely, with assumptions", "No"))
df1$all6_factor <- factor(df1$all6_factor, levels = c("Yes", "No"))
df1$alpha_factor <- factor(df1$alpha_factor, levels = c("0.05", "Other", "Not reported"))
df1$power_factor <- factor(df1$power_factor, levels = c("0.80", "0.95", "Other", "Not reported"))
df1$effect_size_type_factor <- factor(df1$effect_size_type_factor, levels = c("d", "f", "Non-standardized", "Other", "Not reported"))
df1$effect_size_value_factor <- factor(df1$effect_size_value_factor, levels = c("Reported", "Not reported"))
df1$stat_test_factor <- factor(df1$stat_test_factor, levels = c("ANOVA", "t-test", "Other", "Not reported"))
df1$sample_size_factor <- factor(df1$sample_size_factor, levels = c("Reported", "Not reported"))
df1$mult_compare_factor <- factor(df1$mult_compare_factor, levels = c("Yes", "No, and multiple analyses are performed", "No, but a single outcome was identified", "Unsure"))
df1$error_factor <- factor(df1$error_factor, levels = c("Yes", "No", "Unsure"))
df1$match_factor <- factor(df1$match_factor, levels = c("Yes", "No", "Unsure", "NA (protocol)"))


# create dataframe with the information to present in a table
t_repro <- bind_rows(table(df1$reproducible_factor) %>% as.data.frame(),
              table(df1$all6_factor) %>% as.data.frame(),
              table(df1$alpha_factor) %>% as.data.frame(),
              table(df1$power_factor) %>% as.data.frame(),
              table(df1$effect_size_type_factor) %>% as.data.frame(),
              table(df1$effect_size_value_factor) %>% as.data.frame(),
              table(df1$stat_test_factor) %>% as.data.frame(),
              table(df1$sample_size_factor) %>% as.data.frame()
) %>% 
  # add columns with the percentage and confidence intevals
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, denom))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, denom))) %>% 
  ungroup()

# make marker numbers for where to break the table using kable
r1 <- nlevels(df1$reproducible_factor)
r2 <- r1 + nlevels(df1$all6_factor)
r3 <- r2 + nlevels(df1$alpha_factor)
r4 <- r3 + nlevels(df1$power_factor)
r5 <- r4 + nlevels(df1$effect_size_type_factor)
r6 <- r5 + nlevels(df1$effect_size_value_factor)
r7 <- r6 + nlevels(df1$stat_test_factor)
r8 <- r7 + nlevels(df1$sample_size_factor)

colnames(t_repro) <- c("Transparency element",
                       paste0("N=", denom),
                       "Percent",
                       "(95% CI)"
                       )

median_sample_size <- median(df1$sample_size_text, na.rm = TRUE)
IQR_low_sample_size <- quantile(df1$sample_size_text, 0.25, na.rm = TRUE) 
IQR_high_sample_size <- quantile(df1$sample_size_text, 0.75, na.rm = TRUE) %>% round(0)

```

```{r, table_reproducibility, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(t_repro, 
             caption = "Table 3. Reproducibility of sample size calculations from a random sample of publications referencing G*Power", 
             booktabs = T, 
             linesep = "\\addlinespace", 
             align = "lrrr") %>%
kable_styling() %>%
  pack_rows("Reproducible", 1, r1) %>%
  pack_rows("All 6 elements reported†", r1+1, r2) %>%
  pack_rows("Alpha", r2+1, r3) %>%
  pack_rows("Power", r3+1, r4) %>%
  pack_rows("Effect size type", r4+1, r5) %>%
  pack_rows("Effect size value", r5+1, r6) %>%
  pack_rows("Statistical test", r6+1, r7) %>%
  pack_rows("Sample size‡", r7+1, r8) %>%
  kable_styling(latex_options = "striped") %>% 
  add_footnote(paste0("*We use the term 'likely' because we cannot be certain that all of our assumptions were correct. †These six elements are: alpha, power or beta, effect size type, effect size vale, statistical test, and sample size. We considered statistical test reported if they named the general test, even if details were missing (e.g., reporting an ANOVA, but not what type of ANOVA. ‡The median sample size was ", median_sample_size, " (IQR: ", IQR_low_sample_size, " to ", IQR_high_sample_size, ")."), notation = "none", threeparttable = T)
```

```{r characteristics}
# create a table with the article characteristics (in other words, the typical "Table 1" of many manuscripts)
t_char <- bind_rows(
              table(df1$participants) %>% as.data.frame(),
              table(df1$pub_year) %>% as.data.frame(),
              table(df1$protocol) %>% as.data.frame() %>% arrange(Freq),
              table(df1$multiple) %>% as.data.frame() %>% arrange(Freq),
              ) %>% 
  mutate(perc = paste0((Freq / denom) %>% round(2) * 100, "%")
  )

r1 <- nrow(table(df1$participants))
r2 <- r1 + nrow(table(df1$pub_year))
r3 <- r2 + nrow(table(df1$protocol))
r4 <- r3 + nrow(table(df1$multiple))

colnames(t_char) <- c("Article characteristic",
                       paste0("N=", denom),
                       "Percent"
                       )

median_impact_factor <- median(df1$impact_factor, na.rm = TRUE) %>% round(1)
min_impact_factor <- min(df1$impact_factor, na.rm = TRUE) %>% round(1)
max_impact_factor <- max(df1$impact_factor, na.rm = TRUE) %>% round(1)
n_no_impact_factor <- sum(is.na(df1$impact_factor))

pub <- table(df1$publisher) %>%
  as.data.frame() %>% 
  arrange(desc(Freq))

colnames(pub) <- c("publisher", "Freq")


# clean up this code to output the publisher counts.
sum(df1$publisher == "BMC")
sum(df1$publisher == "MDPI")
sum(df1$publisher == "FRONTIERS MEDIA SA")
sum(df1$publisher == "SPRINGER")
sum(df1$publisher == "WILEY")

pub_other <- denom - (sum(df1$publisher == "BMC") +
sum(df1$publisher == "MDPI") +
sum(df1$publisher == "FRONTIERS MEDIA SA") +
sum(df1$publisher == "SPRINGER") +
sum(df1$publisher == "WILEY")
)

x <- nrow(pub) - 5

```

```{r, table_characteristics, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(t_char, 
             caption = "Supplementary Table 1. Article characteristics", 
             booktabs = T, 
             linesep = "\\addlinespace", 
             align = "lrr") %>%
kable_styling() %>%
  pack_rows("Unit of study", 1, r1) %>%
  pack_rows("Year of publication", r1+1, r2) %>%
  pack_rows("Protocol", r2+1, r3) %>%
  pack_rows("Multiple sample size calculations", r3+1, r4) %>%
  kable_styling(latex_options = "striped") %>% 
  add_footnote(paste0("The median journal impact factor was ", median_impact_factor, " (range: ", min_impact_factor, " to ", max_impact_factor, "). ", n_no_impact_factor, " articles were pubished in journals that did not have an impact factor."), notation = "none", threeparttable = T)
```

```{r quality}
# make a dataframe of the justification, so they are all binary, ordered from most common to least common
justification <- data.frame(
  Var1 = c(
          "Previously published research",
          "Effect size conventions",
          "General reference to another study",
          "Pilot data",
          "Effect size of interest",
          "Other",
          "No justification reported"
      ),
  Freq = c(
          sum(grepl("Previously published research", df1$justification)),
          sum(grepl("Conventions", df1$justification)),
          sum(grepl("Reference to other study to justify their calculation in general", df1$justification)),
          sum(grepl("pilot data", df1$justification)),
          sum(grepl("MCID", df1$justification)),
          sum(grepl("Other", df1$justification)),
          sum(grepl("No justification", df1$justification))
        )
)

# combine all quality factors into a single dataframe
t_qual <- bind_rows(table(df1$match_factor) %>% as.data.frame(),
                    table(df1$error_factor) %>% as.data.frame(),
                    table(df1$mult_compare_factor) %>% as.data.frame(),
                    justification
                    ) %>% 
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, denom))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, denom))) %>% 
  ungroup()

# index for kable function
r1 <- nlevels(df1$match_factor)
r2 <- r1 + nlevels(df1$error_factor)
r3 <- r2 + nlevels(df1$mult_compare_factor)
r4 <- r3 + nrow(justification)

# replace the ci for the one element with 0 hits with NA
i <- grepl("- <1%", t_qual$ci) %>% 
  which()
t_qual$ci[i] <- "(NA)"

colnames(t_qual) <- c("Quality measure",
                       paste0("N=", denom),
                       "Percent",
                       "(95% CI)"
                       )

```

```{r, table_qual, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(t_qual, 
             caption = "Table 4. Measures of quality of the sample size calculations", 
             booktabs = T, 
             linesep = "\\addlinespace",
             align = "lrrr") %>%
kable_styling() %>%
  pack_rows("Analysis match", 1, r1) %>%
  pack_rows("Error", r1+1, r2) %>%
  pack_rows("Adjusted for multiple comparisons", r2+1, r3) %>%
  pack_rows("Justification for chosen effect size", r3+1, r4) %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote("Some articles provided more than one justification for their chosen sample size, and thus the sum of the percentages exceeds 100%.", notation = "none", threeparttable = T)
```

```{r kappa}
inter_rater <- function(df, variables, prescreen_questions){
  
  # Initialize an empty list to store results
    kappa_results <- data.frame(matrix(nrow = length(variables), ncol = 5))
    colnames(kappa_results) <- c("k", "percent_agreed", "n_disagreed", "n_total", "n_categories")
    rownames(kappa_results) <- variables
    
  # Loop through each variable
  for (var in variables) {
    
    if(prescreen_questions == TRUE){
    # Subset data for each coder and specific variable
    data1 <- subset(df, coder == "RTT")[, var]
    data2 <- subset(df, coder == "EZ")[, var]
    } else {
    data1 <- subset(df, coder_number == 1)[, var]
    data2 <- subset(df, coder_number == 2)[, var]
    }
      
    # Combine into a matrix
    combined_data <- matrix(c(data1, data2), ncol = 2)
    
    # Calculate Cohen's Kappa
    result <- kappa2(combined_data)
    
    # Store the result in the list
    kappa_results[[var, "k"]] <- result$value %>% round(2)
    kappa_results[[var, "percent_agreed"]] <- (sum(data1 == data2, na.rm = TRUE) / result$subjects) %>% round(2)
    kappa_results[[var, "n_disagreed"]] <- sum(data1 != data2, na.rm = TRUE)
    kappa_results[[var, "n_total"]] <- result$subjects
    kappa_results[[var, "n_categories"]] <- n_distinct(df[[var]])
  }
  return(kappa_results)
}


# select variables to calculate inter-rater agreement for when looking at all articles we assessed (both included and excluded articles). The variables it doesn't make sense to calculate agreement for are commented out.
variables1 <- c(
  "id",
#  "coder",
#  "coder_text",
#  "coder_numer",
  "pmcid",
  "protocol",
  "include"
)

kappa_include <- inter_rater(df, variables1, TRUE)

# identify ids for included articles
id_include <- df %>% filter(grepl("/", coder)) %>% 
  filter(include == "Include") %>% 
  select(id)

# remove excluded articles based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df_kappa <- df %>% filter(id %in% id_include$id)

# now that I've removed the excluded articles, I can calculate inter-rater agreement for the variables coded for all included article. All these questions were answered before the survey allowed the coder to select the option "I don't have the expertise for this article".

# select variables to calculate inter-rater agreement for when looking at all included articles. The variables it doesn't make sense to calculate agreement for are commented out.
variables2 <- c(
  "participants",
#  "participants_text",
  "journal", # small difference in typing may result in different coding
#  "publisher", # recorded by one coder after all other coding was done (thus cannot calculate IRR)
  "pub_year",
  "impact_factor",
#  "verbatim",
  "power_calc_type",
# "power_calc_type_text",
  "multiple"
#  "stats_knowledge",
)

# Recode NAs to -100 in the "impact_factor" column
df_kappa$impact_factor <- ifelse(is.na(df_kappa$impact_factor), -100, df_kappa$impact_factor)

kappa_any_power_calc <- inter_rater(df_kappa, variables2, TRUE)

# return the impact factors to NA. I only needed them to be a number for the n_disagreement variable
# df$impact_factor <- ifelse(is.na(df$impact_factor), -100, df$impact_factor)

# identify ids for articles not solving for sample size
id_sample_size_calc <- df %>% filter(grepl("/", coder)) %>% 
  filter(power_calc_type == "Solves for sample size (often called a priori)") %>% 
  select(id)

# remove articles that don't solve for sample size based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df_kappa <- df_kappa %>% filter(id %in% id_sample_size_calc$id)

# calculate inter-rater agreement for all relevant variables

# select variables to calculate inter-rater agreement for when looking only at articles with a sample size calculation. The variables it doesn't make sense to calculate agreement for are commented out.
variables3 <- c(
  "version",
  "version_text",
  "power",
  "power_text",
  "alpha",
  "alpha_text",
  "sample_size",
  "sample_size_text",
  "effect_size_type",
#  "effect_size_type_other_standardized_text",
#  "effect_size_type_other_nonstandardized_text",
  "effect_size_value",
  "effect_size_value_text",
  "stat_test",
#  "stat_test_other_regression_text",
#  "stat_test_other_nonregression_text",
#  "other_info_missing",
  "reproducible",
#  "reproducible_text",
  "justification",
  "just_previous",
  "just_pilot",
  "just_convention",
  "just_mcid",
  "just_none",
  "just_ref",
  "just_other",
# "justification_text",
  "mult_compare",
#  "mult_compare_text",
  "anova_within_between",
  "match",
#  "match_text",
  "error"
#  "error_text",
#  "impact",
#  "comments_calc",
#  "comments_general",
#  "posthoc_resolving_notes"
)

kappa_sample_size_calc <- inter_rater(df_kappa, variables3, FALSE)

# merge all inter-rater agreement scores
kappa_all <- rbind(kappa_include,
                   kappa_any_power_calc,
                   kappa_sample_size_calc
)

kappa_all <- kappa_all %>% 
  mutate(percent_agreed = paste0(percent_agreed * 100, "%"))

```

```{r, table_kappa, include = TRUE, echo = FALSE, results = "asis"}
  knitr::kable(kappa_all, 
               caption = "Supplementary Table 2. Inter-rater agreement", 
               booktabs = T, 
               linesep = "\\addlinespace",
               align = "lrrrrr",
               col.names = c("Variable",
                         "Cohen's k",
                         "Percent agreed",
                         "n disagreed",
                         "n total",
                         "n categories"
                         )) %>%
    kable_styling(latex_options = "striped") %>%
    add_footnote("The variables are listed as they appear in the open data. See the data dictionary for a description of each variable. Cohen's kappa is mostly irrelevant for variables with a large number of categories, and can be ignored. Not all variables were relevant for all articles we coded; thus, 'n total' differs among the variables. 'justification was coded as a radio question with 7 options. We re-coded this variable into 7 binary variables and calculate the inter-rater agreement for each one.", notation = "none", threeparttable = T)
```

```{r anova}

filename <- paste0(getwd(), "/data/data_anova_raw.csv")
raw_anova <- read.csv(filename, header = T)
df_anova_new <- raw_anova %>% 
  select(-c(coder_text, participants_text)
         )

df1_anova_new <- df_anova_new %>% filter(grepl("/", coder))

df1_anova_original <- df1 %>% select(c(id,	
                                       coder,	
                                       pmcid,
                                       protocol,	
                                       participants,	
                                       journal,	pub_year,	
                                       impact_factor,	
                                       verbatim,	
                                       reproducible,	
                                       reproducible_text,	
                                       anova_within_between,	
                                       comments_calc,	
                                       comments_general
                                       )
) %>% 
  filter(grepl("Yes, ", anova_within_between)
  )

df1_anova_new$pub_year <- as.character(df1_anova_new$pub_year)

df1_anova <- bind_rows(df1_anova_original, df1_anova_new)


anova_results <- data.frame(
  item = c(
    "Non-default option",
    "Default option",
    "Unsure"
  ),
  Freq = c(
    sum(grepl("Yes, and the researchers selected a non-default option.", df1_anova$anova_within_between)),
    sum(grepl("Yes, but the researchers use the default option.", df1_anova$anova_within_between)),
    sum(grepl("Yes, but I cannot reasonably assume which option they used.", df1_anova$anova_within_between))
  )
) %>% 
  rowwise() %>% 
  mutate(point = point_percent(ci_calc(Freq, nrow(df1_anova)))) %>% 
  mutate(ci = ci_percent(ci_calc(Freq, nrow(df1_anova)))) %>% 
  ungroup()

```

```{r, table_anova, include = TRUE, echo = FALSE, results = "asis"}
  knitr::kable(anova_results, 
               caption = "Table 100. ANOVA outcomes", 
               booktabs = T, 
               linesep = "\\addlinespace",
        #       align = "lrrrrr",
               col.names = c(
                 "Option used",
                 "N=37",
                 "Percent",
                 "(95% CI)"
               )
  ) %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote("temp", notation = "none", threeparttable = T)
```

```{r REC}
# This chuck outputs all the results for the Research Ethics Committee Project

# Import data
filename <- paste0(getwd(), "/data/REC_data.csv")
rec <- read.csv(filename, header = T)

# Remove irrelevant variables
rec <- rec %>% select(-c(database_link, database_number))

# Create dataframes for only US and only UK institutions
us <- rec %>% filter(country == "us" & !is.na(expenditure_rank))
uk <- rec %>% filter(country == "uk" & !is.na(expenditure_rank))

# Create a function to output all the values for the flowchart
rec_flowchart <- function(df){ 
  flowchart <- data.frame(
    item = c(
      "Institutions sampled",
      "Public forms identified",
      "Institutions contacted",
      "Public forms identified after contact",
      "Private forms provided",
      "Forms indicated as not available to share", 
      "No response"
    ),
    count = c(
      nrow(df),
      sum(df$included == 1 & df$contacted == 0),
      sum(df$contacted == T),
      sum(df$contacted == 1 & df$form_available == "public", na.rm = T),
      sum(sum(df$contacted == 1 & df$form_available == "private", na.rm = T)),
      sum(df$form_available == "declined", na.rm = T),
      sum(df$form_available == "noreply" | is.na(df$form_available))
    )
  )
  return(flowchart)
}

# Create a function to output all the values for the outcomes
rec_outcomes <- function(df){
  outcomes <- data.frame(
    item = c(
      "Sample size",
      "Justification",
      "Calculation",
      "Calculation (indirectly)"
    ),
    count = c(
      sum(df$sample_size, na.rm = TRUE),
      sum(df$justification, na.rm = TRUE),
      sum(df$calculation == 1, na.rm = TRUE),
      sum(df$calculation == 99, na.rm = TRUE)
    )
  )
  return(outcomes)
}

us_flowchart <- rec_flowchart(us)
uk_flowchart <- rec_flowchart(uk)

us_outcomes <- rec_outcomes(us)
uk_outcomes <- rec_outcomes(uk)

us_combined <- bind_rows(
  rec_flowchart(us),
  rec_outcomes(us)
  )

uk_combined <- bind_rows(
  rec_flowchart(uk),
  rec_outcomes(uk)
  )

rec_results <- cbind(
  us_combined, 
  uk_combined$count
)
```

```{r, table_rec, include = TRUE, echo = FALSE, results = "asis"}
  knitr::kable(rec_results, 
               caption = "Table 101. REC results", 
               booktabs = T, 
               linesep = "\\addlinespace",
        #       align = "lrrrrr",
               col.names = c(
                 "Flowchart item",
                 "US (n)",
                 "UK (n)"
               )
  ) %>%getwd
  kable_styling(latex_options = "striped") %>%
  add_footnote("temp", notation = "none", threeparttable = T)
```
