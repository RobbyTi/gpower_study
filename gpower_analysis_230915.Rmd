---
title: "gpower_analysis_221005"
author: "Robert Thibault"
date: "05/10/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE) # this option stops the code chunks from being output from the knit
```

```{r packages}
library(tidyverse) # for cleaner code
library(knitr) # for kable function
library(kableExtra) # for kable table styling
library(irr) # for inter-rater agreement calculations
library(english) # to convert numbers to word equivalents via the function: words() 
library(statpsych) # for the precision analysis
library(table1) # for making Table 1
```

```{r clean_data}
# load the raw data
filepath <- paste0(getwd(), "/gpower_dataset_230915.csv" )
raw <- read.csv(filepath, header = TRUE)
df <- raw

df <-
  df %>% 
  # remove irrelevant columns that Qualtrics exports
  # select(-c(1:18)) %>% 
  # remove empty rows for coders other than RT, EZ, and HP (which there were not)
  select(-coder_text) %>% 
  # filter out resolving notes column
  filter(coder != "resolveNotes") %>% 
  filter(id != "") %>%   # remove irrelevant rows that Qualtrics exports
  filter(id != "156") %>%  # remove extra paper RTT coded
  
  ############# TEMPORARILY REMOVE THESE TWO, UNTIL HUGO CODES THEM
    filter(id != "143") %>%  
    filter(id != "146") %>%  
  ################
  
  # recode the $power_1_TEXT column to 0.80, where relevant. This is necessary as we updated the Qualtrics extraction form to add a button for "Yes (0.80)" to speed up coding
  mutate(
    power_text = if_else(power == "Yes (0.80)", "0.80", power_text),
    power = if_else(power == "Yes (0.80)", "Yes", power),
    # repeat for $alpha column
    alpha_text = if_else(alpha == "Yes (0.05)", "0.05", alpha_text),
    alpha = if_else(alpha == "Yes (0.05)", "Yes", alpha)
  )

# recode Yes/No columns to 1 (yes), 2 (no), and NA
df <- 
  df %>% 
  mutate_at(c("version", "power", "alpha", "sample_size", "effect_size_value"),
      ~ case_when(
      . == "Yes" ~ 1L,
      . == "No" ~ 0L,
      is.na(.) ~ NA_integer_) 
      ) %>% 
  mutate(effect_size_type_binary = 
      case_when(
      effect_size_type == "No" ~ 0L,
      effect_size_type == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  stat_test_binary = 
      case_when(
      stat_test == "the statistical test is NOT reported" ~ 0L,
      stat_test == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  # make new column that checks how many power calcs include all 6 elements
  all6 = (
    power + alpha + sample_size + effect_size_type_binary + effect_size_value + stat_test_binary
    ),
  # recode some columns to binary that weren't previously only "Yes" and "No"
  all6_binary = 
      case_when(
      all6 == 6 ~ 1L,
      all6 < 6 ~ 0L,
      is.na(all6) ~ NA_integer_
      ),
  reproducibileBinary = 
      case_when(
      grepl("Yes", reproducible) ~ 1L,
      reproducible == "No" ~ 0L,
      is.na(reproducible) ~ NA_integer_
      ),
  JustificationBinary = 
      case_when(
      grepl("No justification", justification) ~ 0L,
      is.na(justification) ~ NA_integer_,
      TRUE ~ 0L,
      )
  )

# recode a few columns to numerics
df$id <- as.numeric(df$id)
df$power_text <- as.numeric(df$power_text)
df$alpha_text <- as.numeric(df$alpha_text)
df$sample_size_text <- as.numeric(df$sample_size_text)
df$effect_size_value_text <- as.numeric(df$effect_size_value_text)
df$impact_factor <- as.numeric(df$impact_factor)


inter_rater <- function(df, variables, prescreen_questions){
  
  # Initialize an empty list to store results
    kappa_results <- data.frame(matrix(nrow = length(variables), ncol = 5))
    colnames(kappa_results) <- c("k", "percent_agreed", "n_disagreed", "n_total", "n_categories")
    rownames(kappa_results) <- variables
    
      # Loop through each variable
  for (var in variables) {
    
    if(prescreen_questions == TRUE){
    # Subset data for each coder and specific variable
    data1 <- subset(df, coder == "RTT")[, var]
    data2 <- subset(df, coder == "EZ")[, var]
    } else {
    data1 <- subset(df, coder_number == 1)[, var]
    data2 <- subset(df, coder_number == 2)[, var]
    }
      
    # Combine into a matrix
    combined_data <- matrix(c(data1, data2), ncol = 2)
    
    # Calculate Cohen's Kappa
    result <- kappa2(combined_data)
    
    # Store the result in the list
    kappa_results[[var, "k"]] <- result$value %>% round(2)
    kappa_results[[var, "percent_agreed"]] <- (sum(data1 == data2, na.rm = TRUE) / result$subjects) %>% round(2)
    kappa_results[[var, "n_disagreed"]] <- sum(data1 != data2, na.rm = TRUE)
    kappa_results[[var, "n_total"]] <- result$subjects
    kappa_results[[var, "n_categories"]] <- n_distinct(df[[var]])
  }
  return(kappa_results)
}

variables1 <- c(
  "id",
#  "coder",
#  "coder_text",
#  "coder_numer",
  "pmcid",
  "protocol",
  "include"
)

kappa_include <- inter_rater(df, variables1, TRUE)

# identify ids for included articles
id_include <- df %>% filter(grepl("/", coder)) %>% 
  filter(include == "Include") %>% 
  select(id)

# remove excluded articles based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df <- df %>% filter(id %in% id_include$id)

# now that I've removed the excluded articles, I can calculate inter-rater agreement for the variables coded for all included article. All these questions were answered before the survey allowed the coder to select the option "I don't have the expertise for this article".
variables2 <- c(
  "participants",
#  "participants_text",
  "journal", # small difference in typing may result in different coding
#  "publisher", # recorded by one coder after all other coding was done (thus cannot calculate IRR)
  "pub_year",
  "impact_factor",
#  "verbatim",
  "power_calc_type",
# "power_calc_type_text",
  "multiple"
#  "stats_knowledge",
)

# Recode NAs to -100 in the "impact_factor" column
df$impact_factor <- ifelse(is.na(df$impact_factor), -100, df$impact_factor)

kappa_any_power_calc <- inter_rater(df, variables2, TRUE)

# return the impact factors to NA. I only needed them to be a number for the n_disagreement variable
df$impact_factor <- ifelse(is.na(df$impact_factor), -100, df$impact_factor)

# identify ids for articles not solving for sample size
id_sample_size_calc <- df %>% filter(grepl("/", coder)) %>% 
  filter(power_calc_type == "Solves for sample size (often called a priori)") %>% 
  select(id)

# remove articles that don't solve for sample size based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df <- df %>% filter(id %in% id_sample_size_calc$id)

# calculate inter-rater agreement for all relevant variables

# List of variables to check
variables3 <- c(
  "version",
  "version_text",
  "power",
  "power_text",
  "alpha",
  "alpha_text",
  "sample_size",
  "sample_size_text",
  "effect_size_type",
#  "effect_size_type_other_standardized_text",
#  "effect_size_type_other_nonstandardized_text",
  "effect_size_value",
  "effect_size_value_text",
  "stat_test",
#  "stat_test_other_regression_text",
#  "stat_test_other_nonregression_text",
#  "other_info_missing",
  "reproducible",
#  "reproducible_text",
  "justification",
# "justification_text",
  "mult_compare",
#  "mult_compare_text",
  "anova_within_between",
  "match",
#  "match_text",
  "error"
#  "error_text",
#  "impact",
#  "comments_calc",
#  "comments_general",
#  "posthoc_resolving_notes"
)

kappa_sample_size_calc <- inter_rater(df, variables3, FALSE)

kappa_all <- rbind(kappa_include,
                   kappa_any_power_calc,
                   kappa_sample_size_calc
)

```

#################### coded to here ################3





# make a new dataframe that only includes the filtered data, using with the keyword/character "/"
d1_all <- d %>% filter(grepl("/", coder))

# filter for articles that use GPower to solve for sample size
d1 <- d1_all %>% filter(grepl("Solves for sample size", powerCalcType))

#assign integer to use to calculate percentages
denom <- nrow(d1)

```

```{r functions}

# create function to calculate the point estimates and confidence intervals based on Monte Carlo sampling
ciCalc <- function(num, denom){ 
     sim <- rbeta(10000, shape1=num, shape2=(denom-num)) # simulate the distibution
     point <- (quantile(sim, probs=0.5)) # calculate the point estimate and 95% CI bounds
     lb <- (quantile(sim, probs=0.025))
     ub <- (quantile(sim, probs=0.975))
     cis <- c(point, lb, ub)
  return(cis)
}

#create function to create point estimates and confidence intervals as they'll appear in the table output
ciClean <- function(ci){
  ci <- ci %>% round(2)
  ciOut <- paste0(ci[1], " (", ci[2], " - ", ci[3], ")")
  return(ciOut)
}

```

```{r tableArticles}
# this chunk estimates the total number of published articles that use GPower for doing (1) a power calculation, or (2) an a priori sample size calculation

denomAll <- nrow(d1_all) # number of articles coded
include <- sum(d1_all$include == "Include")  # number of articles with any power calc
ANOVA <- sum(grepl("Yes", d1$ANOVAwithinBetween)) # number of the 120 articles that used GPower for an ANOVA sample size calculation
pmcTotal = 3285893 # total number of PMC articles published in the time of our query: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pmcHits = 22188  # number of search hits from our query (GPower OR “G Power”) AND ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pubmedTotal = 7318980 # total number of PubMed articles published in that period: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [Date - Entry] : "2022/05/31" [Date - Entry])
proportion <- pmcHits/pmcTotal # proportion of PMC articles that were hits
multFact <- pubmedTotal/pmcTotal # the factor to multiply our findings for PMC articles to get an estimate for all PubMed articles.
multFact50 <- 1 + (multFact -1)/2 # a conservative multiplication factor if we assume that half as many papers in pubmed use power calculations as compared to PMC

#initialize dataframe for point estimates and confidence intervals
cis <- data.frame(matrix(nrow=6, ncol=1))


# create function to calculate the point estimates and confidence intervals based on Monte Carlo sampling
ciCalc <- function(total, proportion, num, denom){ 
     sim <- rbeta(10000, shape1=num, shape2=(denom-num)) # simulate the distibution
     point <- (quantile(sim, probs=0.5)) # calculate the point estimate and 95% CI bounds
     lb <- (quantile(sim, probs=0.025))
     ub <- (quantile(sim, probs=0.975))
     cis <- c(point, lb, ub)
     cis <- cis * total * proportion
  return(cis)
}


# run the function to calculate point estimates and confidence intervals 
cis <- as.data.frame(
  cbind(ciCalc(pmcTotal, proportion, include, denomAll),
             ciCalc(pmcTotal, proportion, denom, denomAll),
             ciCalc(pmcTotal, proportion, ANOVA, denomAll)
             )
)
colnames(cis) <- c("Any power calculation",
                   "Sample size calculation",
                   "ANOVA sample size calculation"
)

cisRound <- cis %>% round(-3)

#create function to create point estimates and confidence intervals as they'll appear in the table output
ciClean <- function(ci){
  ci <- ci %>% round(-2)
  ciOut <- paste0(ci[1], " (", ci[2], " - ", ci[3], ")")
  return(ciOut)
}

cisOut <- 
  cis %>% 
  summarise_all(ciClean) %>% 
  t()

```

```{r tableCalcTypes}

calcType <- rbind(
  sum(grepl("sample size", d1_all$powerCalcType)),
  sum(grepl("power", d1_all$powerCalcType)),
  sum(grepl("effect size", d1_all$powerCalcType)),
  sum(grepl("Other", d1_all$powerCalcType)),
  sum(grepl("Unsure", d1_all$powerCalcType))
)
  
rownames(calcType) <- c("Sample  size",
                        "Power",
                        "Effect size",
                        "Sample size, after completing the study",
                        "Unsure"
                        )


```

```{r tableCharacteristics} 
# make Table 1 with article characteristics

# recode the $participants column
d1$participants <- factor(d1$participants, 
       levels = c("Humans", "Non-human animals (this includes both in vivo and in vitro studies)"),
       labels = c("Human", "Non-human animal")
       )

# give the columns labels so that Table 1 is cleaner
label(d1$protocol) <- "Protocol"
label(d1$pubYear) <- "Year of publication"
label(d1$impactFactor) <- "Journal Impact Factor"
label(d1$participants) <- "Unit of study"
label(d1$multiple) <- "Multiple sample size calculations"

# make table 1
t1 <- table1(~ protocol +
               pubYear +
               impactFactor +
               participants +
               multiple,
               data = d1
)

```

```{r tableOutcomes}
# recode columns to have fewer factors that will fit more cleanly into a table
d1 <- d1 %>% 
  mutate(alphaFactor = 
      case_when(
      alpha_1_TEXT == 0.05 ~ "0.05",
      is.na(alpha_1_TEXT) ~ "Not reported",
      TRUE ~ "Other"
      ),
  powerFactor = 
      case_when(
      power_1_TEXT == 0.8 ~ "0.80",
      power_1_TEXT == 0.95 ~ "0.95",
      is.na(power_1_TEXT) ~ "Not reported",
      TRUE ~ "Other"
      ),
  effectSizeTypeFactor = 
      case_when(
      effectSizeType == "d" ~ "d",
      effectSizeType == "f" ~ "f",
      effectSizeType == "Other: Non-standardized (e.g., 5 points on a questionnaire scale)." ~ "Non-standardized",
      effectSizeType == "No" ~ "Not reported",
      TRUE ~ "Other"
      ),
  statTestFactor = 
      case_when(
      statTest == "ANOVA" ~ "ANOVA",
      statTest == "t-test" ~ "t-test",
      statTest == "the statistical test is NOT reported" ~ "Not reported",
      TRUE ~ "Other"
      ),
  reproducibleFactor = 
      case_when(
      reproducible == "Yes, based solely on the information in the article or its supplementary material" ~ "Yes, without assumptions",
      reproducible== "Yes, but I've had to make some assumptions. (please list the assumptions you made)" ~ "Likely, with assumptions",
      reproducible == "No" ~ "No"
      ),
  multCompareFactor = 
      case_when(
      grepl("contains multiple analyses", multCompare) ~ "No, and multiple analyses are performed",
      grepl("no reason to account for multiple comparisons", multCompare) ~ "No, but a single outcome is identified",
      multCompare == "Unsure" ~ "Unsure"
      ),
  errorFactor = 
      case_when(
      grepl("Likely", error) ~ "Unsure",
      grepl("Unsure", error) ~ "Unsure",
      error == "No" ~ "No",
      error == "Yes" ~ "Yes" 
      ),
  matchFactor = 
      case_when(
      grepl("Unsure", match) ~ "Unsure",
      match == "No" ~ "No",
      match == "Yes" ~ "Yes",
      grepl("protocol", match) ~ "NA, protocol"
      )
  )

# order the levels of the factors for clarity
d1$alphaFactor <- factor(d1$alphaFactor, levels = c("0.05", "Other", "Not reported"))
d1$powerFactor <- factor(d1$powerFactor, levels = c("0.80", "0.95", "Other", "Not reported"))
d1$effectSizeTypeFactor <- factor(d1$effectSizeTypeFactor, levels = c("d", "f", "Non-standardized", "Other", "Not reported"))
d1$statTestFactor <- factor(d1$statTestFactor, levels = c("ANOVA", "t-test", "Other", "Not reported"))
d1$reproducibleFactor <- factor(d1$reproducibleFactor, levels = c("Yes, without assumptions", "Likely, with assumptions", "No"))
d1$multCompareFactor <- factor(d1$multCompareFactor, levels = c("No, and multiple analyses are performed", "No, but a single outcome is identified", "Unsure"))
d1$errorFactor <- factor(d1$errorFactor, levels = c("Yes", "No", "Unsure"))
d1$matchFactor <- factor(d1$matchFactor, levels = c("Yes", "No", "Unsure", "NA, protocol"))

# label the columns so that Table 2 is cleaner
label(d1$alphaFactor) <- "Alpha"
label(d1$powerFactor) <- "Power"
label(d1$effectSizeTypeFactor) <- "Effect size type"
label(d1$statTestFactor) <- "Statistical test"
label(d1$sampleSize_1_TEXT) <- "Sample size"
label(d1$reproducibleFactor) <- "Reproducible"
label(d1$multCompareFactor) <- "Adjusted for multiple comparisons"
label(d1$errorFactor) <- "Error"
label(d1$matchFactor) <- "Analysis match"

t2 <- table1(~ alphaFactor + 
               powerFactor + 
               effectSizeTypeFactor + 
               statTestFactor +
               sampleSize_1_TEXT +
               reproducibleFactor +
               multCompareFactor +
               errorFactor + 
               matchFactor,
               data = d1
)

# Some power calculations had multiple justifications, so we need separate code for this column
justification <- rbind(
  sum(grepl("Previously published research", d1$justification)),
  sum(grepl("pilot data", d1$justification)),
  sum(grepl("Conventions", d1$justification)),
  sum(grepl("MCID", d1$justification)),
  sum(grepl("No justification", d1$justification)),
  sum(grepl("Reference to other study to justify their calculation in general", d1$justification)),
  sum(grepl("Other", d1$justification))
)

rownames(justification) <- c("Previous published research",
                             "Pilot data",
                             "Effect size conventions",
                             "Effect size of interest",
                             "No justification provided",
                             "Reference to another study, but not specifically to effect size",
                             "Other"
                             )


```

```{r effectSizes}
# summarize the values for the 15 power calcs that were done with Cohen's d and the 15 with Cohen's f
dCohend <- d1 %>% 
  filter(effectSizeType == "d")
cohend <- summary(dCohend$effectSizeValue_1_TEXT)

dCohenf <- d1 %>% 
  filter(effectSizeType == "f")
cohenf <- summary(dCohenf$effectSizeValue_1_TEXT)

```

```{r}
tab <- 
  d1 %>% 
  summarize_at(c("version", 
                 "power", 
                 "alpha", 
                 "sampleSize", 
                 "effectSizeTypeBinary", 
                 "effectSizeValue", 
                 "statTestBinary", 
                 "all6Binary"), 
  ~ ciCalc(sum(.), denom))

```

```{r}
t1
             

ciPower <- ciCalc(sum(d1$power), denom)
ciAlpha <- ciCalc(sum(d1$alpha), denom)
ciSampleSize <- ciCalc(sum(d1$sampleSize), denom)
ciEffectSizeType <- ciCalc(sum(d1$effectSizeType != "No", na.rm = T), denom)
ciEffectSizeValue <- ciCalc(sum(d1$effectSizeValue), denom)
ciEffectSizeValue <- ciCalc(sum(d1$effectSizeValue), denom)


# MArton suggestion for making a table
# sum_table <- 
#   d1 %>% 
#   summarise(
#     ciPower = ciCalc(sum(power), denom),
# ciAlpha = ciCalc(sum(d1$alpha), denom),
# ciSampleSize = ciCalc(sum(d1$sampleSize), denom),
# ciEffectSizeType = ciCalc(sum(d1$effectSizeType != "No", na.rm = T), denom),
# ciEffectSizeValue = ciCalc(sum(d1$effectSizeValue), denom),
# ciEffectSizeValue = ciCalc(sum(d1$effectSizeValue), denom),
#   )


ciPowerOut <- ciClean(ciPower)



```{r, Table1, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(cisOut, caption = "Table 1. Estimates of the number of published articles that use GPower", booktabs = T, linesep = "", align = "c") %>% 
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  kable_styling(font_size = 8) %>% 
    add_footnote(paste0("The table is divided into articles that use G*Power for: any power calculation related to any statistical test (Any power calculation), a power calculation for any statistical test that solves for sample size (Sample size calculation), and a power calculation for an ANOVA that solves for sample size (ANOVA sample size calculation). The total number of articles in each database since 2017 is: PubMed Central ", prettyNum(pmcTotal, big.mark=",", scientific=F), "; PubMed ", prettyNum(pubmedTotal, big.mark=",", scientific=F), "."), notation = "none", threeparttable = T) 
```






## CODE to get preliminary results to the GPower team

# filter for calculations solving for power
d <- d %>% filter(grepl("Solves for sample size", powerCalcType))


reproducible <- table(d$reproducible)
power <- table(d$power)
alpha <- table(d$alpha)
effectSizeType <- table(d$alpha)
sampleSize <- table(d$sampleSize)
effectSizeValue <- table(d$effectSizeValue)
statTest <- table(d$statTest)
ANOVAwithinBetween <- table(d$ANOVAwithinBetween)
multCompare <- table(d$multCompare)
match <- table(d$match)
error <- table(d$error)



filepath <- paste0(getwd(), "/gpower_codingResolvingANOVAs_220929.csv" )
rawANOVA <- read.csv(filepath, header = TRUE)

dANOVA <- rawANOVA
# filter to only include resolved coding using with the keyword/charcter "/"
dANOVA <- dANOVA %>% filter(grepl("/", coder))
dANOVA <- dANOVA[1:9,]
ANOVAwithinBetweenANOVA <- table(dANOVA$ANOVAwithinBetween)
```

## prelim reproducibility results
Reproducible with assumptions: `r sum(grepl("Yes, but", d$reproducible))`

Reproducible without making assumptions: `r sum(grepl("Yes, based solely", d$reproducible))`

Not reproducible: `r sum(grepl("No", d$reproducible))`

Effect size value not reported: `r sum(grepl("No", d$effectSizeValue))`

Effect size type not reported: `r sum(grepl("No", d$effectSizeType))`

Statistical test not reported: `r sum(grepl("NOT reported", d$statTest))`


```{r repro}

drep <- d %>% filter(grepl("Yes", reproducible))

reproducible_rep <- table(drep$reproducible)
power_rep <- table(drep$power)
alpha_rep <- table(drep$alpha)
effectSizeType_rep <- table(drep$effectSizeType)
sampleSize_rep <- table(drep$sampleSize)
effectSizeValue_rep <- table(drep$effectSizeValue)
statTest_rep <- table(drep$statTest)
ANOVAwithinBetween_rep <- table(drep$ANOVAwithinBetween)
multCompare_rep <- table(drep$multCompare)
match_rep <- table(drep$match)
error_rep <- table(drep$error)


```








