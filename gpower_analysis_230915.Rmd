---
title: "gpower_analysis_230915"
author: "Robert Thibault"
date: "05/10/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = FALSE) # this option stops the code chunks from being output from the knit
```

```{r packages}
library(tidyverse) # for cleaner code
library(knitr) # for kable function
library(kableExtra) # for kable table styling
library(irr) # for inter-rater agreement calculations
library(english) # to convert numbers to word equivalents via the function: words() 
library(statpsych) # for the precision analysis
library(table1) # for making Table 1
```

```{r clean_data}
# load the raw data
filepath <- paste0(getwd(), "/gpower_dataset_230915.csv" )
raw <- read.csv(filepath, header = TRUE)
df <- raw

df <-
  df %>% 
  # remove the columns for the initials of coders other than RTT, EZ, and HP (in the end, we only had these 3 coders)
  select(-coder_text) %>% 
  # filter out the rows labeled "resolveNotes", in which we took some notes when resolving differences in coding.
  filter(coder != "resolveNotes") %>% 
  filter(id != "") %>%   # remove two irrelevant rows that Qualtrics exports
  filter(id != "156") %>%  # remove extra paper RTT coded after we had reached our sample size, but that no one else coded.
  
  ############# TEMPORARILY REMOVE THESE TWO, UNTIL HUGO CODES THEM
    filter(id != "143") %>%  
    filter(id != "146") %>%  
  ################
  
  # recode the $power_text column to 0.80, where relevant. This is necessary as we updated the Qualtrics extraction form to add a button for "Yes (0.80)" to speed up coding
  mutate(
    power_text = if_else(power == "Yes (0.80)", "0.80", power_text),
    power = if_else(power == "Yes (0.80)", "Yes", power),
    # repeat for $alpha column
    alpha_text = if_else(alpha == "Yes (0.05)", "0.05", alpha_text),
    alpha = if_else(alpha == "Yes (0.05)", "Yes", alpha)
  )

# recode Yes/No columns to 1 (yes), 0 (no), and NA
df <- 
  df %>% 
  mutate_at(c("version", "power", "alpha", "sample_size", "effect_size_value"),
      ~ case_when(
      . == "Yes" ~ 1L,
      . == "No" ~ 0L,
      is.na(.) ~ NA_integer_) 
      ) %>% 
  mutate(effect_size_type_binary = 
      case_when(
      effect_size_type == "No" ~ 0L,
      effect_size_type == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  stat_test_binary = 
      case_when(
      stat_test == "the statistical test is NOT reported" ~ 0L,
      stat_test == "" ~ NA_integer_,
      TRUE ~ 1L
      ),
  # make new column that checks how many power calcs include all 6 elements
  all6 = (
    power + alpha + sample_size + effect_size_type_binary + effect_size_value + stat_test_binary
    ),
  # recode some columns to binary that weren't previously only "Yes" and "No"
  all6_binary = 
      case_when(
      all6 == 6 ~ 1L,
      all6 < 6 ~ 0L,
      is.na(all6) ~ NA_integer_
      ),
  reproducibile_binary = 
      case_when(
      grepl("Yes", reproducible) ~ 1L,
      reproducible == "No" ~ 0L,
      is.na(reproducible) ~ NA_integer_
      ),
  justification_binary = 
      case_when(
      grepl("No justification", justification) ~ 0L,
      is.na(justification) ~ NA_integer_,
      TRUE ~ 1L,
      )
  )

# recode a few variables we changed during coding
df <- df %>% 
  mutate(anova_within_between = 
      case_when(
      grepl("the default option", anova_within_between) ~ "Yes, but the researchers use the default option.",
      grepl("non-default option", anova_within_between) ~ "Yes, and the researchers selected a non-default option.",
      TRUE ~ as.character(anova_within_between)  # Keep all other values as they are
      )
  ) %>% 
  mutate(error = 
      case_when(
      grepl("Likely", error) ~ "Unsure (there's not enough information to reasonably code yes or no).",
      TRUE ~ as.character(error)  # Keep all other values as they are
      )
  )

# recode a few columns to numerics
df$power_text <- as.numeric(df$power_text)
df$alpha_text <- as.numeric(df$alpha_text)
df$sample_size_text <- as.numeric(df$sample_size_text)
df$effect_size_value_text <- as.numeric(df$effect_size_value_text)
df$impact_factor <- as.numeric(df$impact_factor)

# decompose the justifications variable
df$just_previous <- ifelse(grepl("Previous", df$justification), 1, 0)
df$just_pilot <- ifelse(grepl("pilot data", df$justification), 1, 0)
df$just_convention <- ifelse(grepl("Convention", df$justification), 1, 0)
df$just_mcid <- ifelse(grepl("MCID", df$justification), 1, 0)
df$just_none <- ifelse(grepl("No justification", df$justification), 1, 0)
df$just_ref <- ifelse(grepl("Reference to other study", df$justification), 1, 0)
df$just_other <- ifelse(grepl("Other", df$justification), 1, 0)

# make a new dataframe that only includes the resolved coding
df1_all <- df %>% filter(grepl("/", coder))

# filter for articles that use GPower to solve for sample size
df1 <- df1_all %>% filter(grepl("Solves for sample size", power_calc_type))

# remove blank rows
df1 <- df1 %>% 
  select(-c(coder_number, participants_text, power_calc_type_text))

write_csv(df1, "dataClean.csv")


```

```{r interrater_agreement}
inter_rater <- function(df, variables, prescreen_questions){
  
  # Initialize an empty list to store results
    kappa_results <- data.frame(matrix(nrow = length(variables), ncol = 5))
    colnames(kappa_results) <- c("k", "percent_agreed", "n_disagreed", "n_total", "n_categories")
    rownames(kappa_results) <- variables
    
      # Loop through each variable
  for (var in variables) {
    
    if(prescreen_questions == TRUE){
    # Subset data for each coder and specific variable
    data1 <- subset(df, coder == "RTT")[, var]
    data2 <- subset(df, coder == "EZ")[, var]
    } else {
    data1 <- subset(df, coder_number == 1)[, var]
    data2 <- subset(df, coder_number == 2)[, var]
    }
      
    # Combine into a matrix
    combined_data <- matrix(c(data1, data2), ncol = 2)
    
    # Calculate Cohen's Kappa
    result <- kappa2(combined_data)
    
    # Store the result in the list
    kappa_results[[var, "k"]] <- result$value %>% round(2)
    kappa_results[[var, "percent_agreed"]] <- (sum(data1 == data2, na.rm = TRUE) / result$subjects) %>% round(2)
    kappa_results[[var, "n_disagreed"]] <- sum(data1 != data2, na.rm = TRUE)
    kappa_results[[var, "n_total"]] <- result$subjects
    kappa_results[[var, "n_categories"]] <- n_distinct(df[[var]])
  }
  return(kappa_results)
}

variables1 <- c(
  "id",
#  "coder",
#  "coder_text",
#  "coder_numer",
  "pmcid",
  "protocol",
  "include"
)

kappa_include <- inter_rater(df, variables1, TRUE)

# identify ids for included articles
id_include <- df %>% filter(grepl("/", coder)) %>% 
  filter(include == "Include") %>% 
  select(id)

# remove excluded articles based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df_kappa <- df %>% filter(id %in% id_include$id)

# now that I've removed the excluded articles, I can calculate inter-rater agreement for the variables coded for all included article. All these questions were answered before the survey allowed the coder to select the option "I don't have the expertise for this article".
variables2 <- c(
  "participants",
#  "participants_text",
  "journal", # small difference in typing may result in different coding
#  "publisher", # recorded by one coder after all other coding was done (thus cannot calculate IRR)
  "pub_year",
  "impact_factor",
#  "verbatim",
  "power_calc_type",
# "power_calc_type_text",
  "multiple"
#  "stats_knowledge",
)

# Recode NAs to -100 in the "impact_factor" column
df_kappa$impact_factor <- ifelse(is.na(df_kappa$impact_factor), -100, df_kappa$impact_factor)

kappa_any_power_calc <- inter_rater(df_kappa, variables2, TRUE)

# return the impact factors to NA. I only needed them to be a number for the n_disagreement variable
# df$impact_factor <- ifelse(is.na(df$impact_factor), -100, df$impact_factor)

# identify ids for articles not solving for sample size
id_sample_size_calc <- df %>% filter(grepl("/", coder)) %>% 
  filter(power_calc_type == "Solves for sample size (often called a priori)") %>% 
  select(id)

# remove articles that don't solve for sample size based on their id. This is necessary to calculate inter-rater agreement for the other variables. 
df_kappa <- df_kappa %>% filter(id %in% id_sample_size_calc$id)

# calculate inter-rater agreement for all relevant variables

# List of variables to check
variables3 <- c(
  "version",
  "version_text",
  "power",
  "power_text",
  "alpha",
  "alpha_text",
  "sample_size",
  "sample_size_text",
  "effect_size_type",
#  "effect_size_type_other_standardized_text",
#  "effect_size_type_other_nonstandardized_text",
  "effect_size_value",
  "effect_size_value_text",
  "stat_test",
#  "stat_test_other_regression_text",
#  "stat_test_other_nonregression_text",
#  "other_info_missing",
  "reproducible",
#  "reproducible_text",
  "justification",
  "just_previous",
  "just_pilot",
  "just_convention",
  "just_mcid",
  "just_none",
  "just_ref",
  "just_other",
# "justification_text",
  "mult_compare",
#  "mult_compare_text",
  "anova_within_between",
  "match",
#  "match_text",
  "error"
#  "error_text",
#  "impact",
#  "comments_calc",
#  "comments_general",
#  "posthoc_resolving_notes"
)

kappa_sample_size_calc <- inter_rater(df_kappa, variables3, FALSE)

kappa_all <- rbind(kappa_include,
                   kappa_any_power_calc,
                   kappa_sample_size_calc
)

## print kappa_all as Table S1

```

```{r, table_kappa, include = TRUE, echo = FALSE, results = "asis"}
  knitr::kable(kappa_all, caption = "Supplementary Table 1. Kappa", booktabs = T, linesep = "\\addlinespace") %>%
    kable_styling(latex_options = "striped") %>%
    add_footnote("footnote to add", notation = "none")
```

```{r functions}

# create function to calculate the point estimates and confidence intervals based on Monte Carlo sampling
ci_calc <- function(num, denom){ 
     sim <- rbeta(10000, shape1=num, shape2=(denom-num)) # simulate the distribution
     point <- (quantile(sim, probs=0.5)) # calculate the point estimate and 95% CI bounds
     lb <- (quantile(sim, probs=0.025))
     ub <- (quantile(sim, probs=0.975))
     cis <- c(point, lb, ub)
  return(cis)
}

#create function to create point estimates and confidence intervals as they'll appear in the table output
ci_clean <- function(ci){
  ci <- ci %>% round(2)
  ciOut <- paste0(ci[1], " (", ci[2], " - ", ci[3], ")")
  return(ciOut)
}

```

```{r tableArticles}
# this chunk estimates the total number of published articles that use GPower for doing (1) a power calculation, or (2) an a priori sample size calculation

denom_all <- nrow(df1_all) # number of articles coded
denom <- nrow(df1) # number or articles with a sample size calc
n_included <- sum(df1_all$include == "Include")  # number of articles with any power calc
n_anova <- sum(grepl("Yes", df1$anova_within_between)) # number of articles with a power calc that solves for sample size for a relevant ANOVA
pmc_total = 3285893 # total number of PMC articles published in the time of our query: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pmc_hits = 22188  # number of search hits from our query (GPower OR “G Power”) AND ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [pmclivedate] : "2022/05/31" [pmclivedate]) 
pubmed_total = 7318980 # total number of PubMed articles published in that period: ("2017/01/01" [Publication Date] : "3000" [Publication Date]) AND ("1000" [Date - Entry] : "2022/05/31" [Date - Entry])
proportion <- pmc_hits/pmc_total # proportion of PMC articles that were hits
mult_fact <- pubmed_total/pmc_total # the factor to multiply our findings for PMC articles to get an estimate for all PubMed articles.
# multFact50 <- 1 + (multFact -1)/2 # a conservative multiplication factor if we assume that half as many papers in pubmed use power calculations as compared to PMC

#initialize dataframe for point estimates and confidence intervals
cis <- data.frame(matrix(nrow=6, ncol=1))

# run the function to calculate point estimates and confidence intervals 
cis <- as.data.frame(
  bind_cols(ci_calc(n_included, denom_all),
            ci_calc(denom, denom_all),
            ci_calc(n_anova, denom_all)
           )
)


cis_pmc <- (cis * pmc_total * proportion) %>% 
  round(-3) %>% 
  summarise_all(ci_clean) %>% 
  t() %>% 
  as.data.frame()

cis_pubmed <- (cis * pubmed_total * proportion) %>% 
  round(-3) %>% 
  summarise_all(ci_clean) %>% 
  t() %>% 
  as.data.frame()


cis_out <- bind_cols(cis_pmc, cis_pubmed)

rownames(cis_out) <- c("Any power calculation",
                       "Sample size calculation",
                       "ANOVA sample size calculation"
                       )

colnames(cis_out) <- c("PubMed Central (95% CI)",
                       "PubMed (95% CI)"
                       )

```
  
```{r, table_numbers, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(cis_out, caption = "Table 1. numer of...", booktabs = T, linesep = "\\addlinespace") %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote(paste0("The table is divided into articles that use G*Power for: any power calculation related to any statistical test (Any power calculation), a power calculation for any statistical test that solves for sample size (Sample size calculation), and a power calculation for an ANOVA that solves for sample size (ANOVA sample size calculation). The total number of articles in each database since 2017 is: PubMed Central ", prettyNum(pmc_total, big.mark=",", scientific=F), "; PubMed ", prettyNum(pubmed_total, big.mark=",", scientific=F), "."), notation = "none")
```

```{r tableCalcTypes}

calc_type <- rbind(
  sum(grepl("sample size", df1_all$power_calc_type)),
  sum(grepl("power", df1_all$power_calc_type)),
  sum(grepl("effect size", df1_all$power_calc_type)),
  sum(grepl("Other", df1_all$power_calc_type)),
  sum(grepl("Unsure", df1_all$power_calc_type))
) %>% 
  as.data.frame()

calc_type <- calc_type %>% 
  rowwise() %>% 
  mutate(ci = ci_clean(ci_calc(V1, n_included))) %>% 
  ungroup()


rownames(calc_type) <- c("Sample  size",
                         "Power",
                         "Effect size",
                         "Sample size (after completing the study)",
                         "Unsure"
)

colnames(calc_type) <- c("n",
                         "percentage (95% CI)"
)

```

```{r, table_calc_type, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(calc_type, caption = "Table 2. Types of calculations...", booktabs = T, linesep = "\\addlinespace") %>%
  kable_styling(latex_options = "striped") %>%
  add_footnote("footnote to add", notation = "none")
```
<!-- 
```{r tableCharacteristics} 
# make Table 1 with article characteristics

# recode the $participants column
df1$participants <- factor(df1$participants, 
                           levels = c("Humans", "Non-human animals (this includes both in vivo and in vitro studies)"),
                           labels = c("Human", "Non-human animal")
)

# give the columns labels so that Table 1 is cleaner
label(df1$protocol) <- "Protocol"
label(df1$pub_year) <- "Year of publication"
label(df1$impact_factor) <- "Journal Impact Factor"
label(df1$participants) <- "Unit of study"
label(df1$multiple) <- "Multiple sample size calculations"

# make table 1
t1 <- table1(~ protocol +
               pub_year +
               impact_factor +
               participants +
               multiple,
             data = df1
)

## print t1 as Table 2.

####testing#####

# t2 <- table1(~ protocol +
#                pub_year +
#                impact_factor +
#                participants +
#                multiple +
#                as.factor(alpha_text) +
#                as.factor(power_text) +
#                effect_size_type +
#                stat_test +
#                reproducible +
#                mult_compare +
#                error +
#                match,
#                data = df1
# )

## print t2 as supplementary material. Maybe clean it up a tad. But it's going to serve to include all the information 

```

```{r tableOutcomes}
# recode columns to have fewer factors that will fit more cleanly into a table
df1 <- df1 %>% 
  mutate(all6_factor = 
           case_when(
             all6 == 6 ~ "Yes",
             TRUE ~ "No"
           ),
         alpha_factor = 
           case_when(
             alpha_text == 0.05 ~ "0.05",
             is.na(alpha_text) ~ "Not reported",
             TRUE ~ "Other"
           ),
         power_factor = 
           case_when(
             power_text == 0.8 ~ "0.80",
             power_text == 0.95 ~ "0.95",
             is.na(power_text) ~ "Not reported",
             TRUE ~ "Other"
           ),
         effect_size_type_factor = 
           case_when(
             effect_size_type == "d" ~ "d",
             effect_size_type == "f" ~ "f",
             effect_size_type == "Other: Non-standardized (e.g., 5 points on a questionnaire scale)." ~ "Non-standardized",
             effect_size_type == "No" ~ "Not reported",
             TRUE ~ "Other"
           ),
         effect_size_value_factor = 
           case_when(
             effect_size_value == 1 ~ "Reported",
             effect_size_value == 0 ~ "Not reported"
           ),  
         stat_test_factor = 
           case_when(
             stat_test == "ANOVA" ~ "ANOVA",
             stat_test == "t-test" ~ "t-test",
             stat_test == "the statistical test is NOT reported" ~ "Not reported",
             TRUE ~ "Other"
           ),
         # sample_size_factor = 
         #     case_when(
         #     sample_size_value == 1 ~ "Reported",
         #     sample_size_value == 0 ~ "Not reported"
         #     ),  
         reproducible_factor = 
           case_when(
             reproducible == "Yes, based solely on the information in the article or its supplementary material" ~ "Yes, without assumptions",
             reproducible== "Yes, but I've had to make some assumptions. (please list the assumptions you made)" ~ "Likely, with assumptions",
             reproducible == "No" ~ "No"
           ),
         mult_compare_factor = 
           case_when(
             grepl("contains multiple analyses", mult_compare) ~ "No, and multiple analyses are performed",
             grepl("no reason to account for multiple comparisons", mult_compare) ~ "No, but a single outcome was identified",
             mult_compare == "Unsure" ~ "Unsure",
             mult_compare == "Yes" ~ "Yes"
           ),
         error_factor = 
           case_when(
             grepl("Likely", error) ~ "Unsure",
             grepl("Unsure", error) ~ "Unsure",
             error == "No" ~ "No",
             error == "Yes" ~ "Yes" 
           ),
         match_factor = 
           case_when(
             grepl("Unsure", match) ~ "Unsure",
             match == "No" ~ "No",
             match == "Yes" ~ "Yes",
             grepl("protocol", match) ~ "NA (protocol)"
           )
  )

# order the levels of the factors for clarity
df1$all6_factor <- factor(df1$all6_factor, levels = c("Yes", "No"))
df1$alpha_factor <- factor(df1$alpha_factor, levels = c("0.05", "Other", "Not reported"))
df1$power_factor <- factor(df1$power_factor, levels = c("0.80", "0.95", "Other", "Not reported"))
df1$effect_size_type_factor <- factor(df1$effect_size_type_factor, levels = c("d", "f", "Non-standardized", "Other", "Not reported"))
df1$effect_size_value_factor <- factor(df1$effect_size_value_factor, levels = c("Reported", "Not reported"))
df1$stat_test_factor <- factor(df1$stat_test_factor, levels = c("ANOVA", "t-test", "Other", "Not reported"))
df1$reproducible_factor <- factor(df1$reproducible_factor, levels = c("Yes, without assumptions", "Likely, with assumptions", "No"))
df1$mult_compare_factor <- factor(df1$mult_compare_factor, levels = c("No, and multiple analyses are performed", "No, but a single outcome was identified", "Unsure", "Yes"))
df1$error_factor <- factor(df1$error_factor, levels = c("Yes", "No", "Unsure"))
df1$match_factor <- factor(df1$match_factor, levels = c("Yes", "No", "Unsure", "NA (protocol)"))

# label the columns so that Table 2 is cleaner
label(df1$all6_factor) <- "All 6 elements reported"
label(df1$alpha_factor) <- "Alpha"
label(df1$power_factor) <- "Power"
label(df1$effect_size_type_factor) <- "Effect size type"
label(df1$effect_size_value_factor) <- "Effect size value"
label(df1$stat_test_factor) <- "Statistical test"
label(df1$sample_size_text) <- "Sample size"
label(df1$reproducible_factor) <- "Reproducible"
label(df1$mult_compare_factor) <- "Adjusted for multiple comparisons"
label(df1$error_factor) <- "Error"
label(df1$match_factor) <- "Analysis match"

# df1$sample_size_text <- as.integer(df1$sample_size_text)
# that line didn't work to get rid of decimal places on "6.00" as the min sample size. 
# add IQR, its probably more meaningful than min and max.


t2 <- table1(~ all6_factor +
               reproducible_factor +
               alpha_factor + 
               power_factor + 
               effect_size_type_factor + 
               effect_size_value_factor +
               stat_test_factor +
               sample_size_text +
               mult_compare_factor +
               error_factor + 
               match_factor,
             data = df1,
             statistic = list(sample_size_text = custom_statistic),
             render.continuous=c(.="Median [IQR]"),
             digits.pct = 0, # to remove decimal places on percentages
             digits = 2 # to remove decimal places on median and IQR
)
t2

# Some power calculations had multiple justifications, so we need separate code for this column
justification <- rbind(
  sum(grepl("Previously published research", df1$justification)),
  sum(grepl("pilot data", df1$justification)),
  sum(grepl("Conventions", df1$justification)),
  sum(grepl("MCID", df1$justification)),
  sum(grepl("No justification", df1$justification)),
  sum(grepl("Reference to other study to justify their calculation in general", df1$justification)),
  sum(grepl("Other", df1$justification))
)

rownames(justification) <- c("Previous published research",
                             "Pilot data",
                             "Effect size conventions",
                             "Effect size of interest",
                             "No justification provided",
                             "Reference to another study, but not specifically to effect size",
                             "Other"
)
df1_repro <- df1 %>% filter(reproducible != "No")



```


```{r effectSizes}
# summarize the values for the 15 power calcs that were done with Cohen's d and the 15 with Cohen's f
dCohend <- df1 %>% 
  filter(effect_size_type == "d")
cohend <- summary(dCohend$effectSizeValue_text)

dCohenf <- df1 %>% 
  filter(effect_size_type == "f")
cohenf <- summary(dCohenf$effectSizeValue_text)

```

```{r}
tab <- 
  df1 %>% 
  summarize_at(c("version", 
                 "power", 
                 "alpha", 
                 "sample_size", 
                 "effect_size_typeBinary", 
                 "effectSizeValue", 
                 "stat_testBinary", 
                 "all6Binary"), 
               ~ ciCalc(sum(.), denom))

```

```{r}
t1


ciPower <- ciCalc(sum(df1$power), denom)
ciAlpha <- ciCalc(sum(df1$alpha), denom)
ciSampleSize <- ciCalc(sum(df1$sample_size), denom)
ciEffectSizeType <- ciCalc(sum(df1$effect_size_type != "No", na.rm = T), denom)
ciEffectSizeValue <- ciCalc(sum(df1$effectSizeValue), denom)
ciEffectSizeValue <- ciCalc(sum(df1$effectSizeValue), denom)


# MArton suggestion for making a table
# sum_table <- 
#   df1 %>% 
#   summarise(
#     ciPower = ciCalc(sum(power), denom),
# ciAlpha = ciCalc(sum(df1$alpha), denom),
# ciSampleSize = ciCalc(sum(df1$sample_size), denom),
# ciEffectSizeType = ciCalc(sum(df1$effect_size_type != "No", na.rm = T), denom),
# ciEffectSizeValue = ciCalc(sum(df1$effectSizeValue), denom),
# ciEffectSizeValue = ciCalc(sum(df1$effectSizeValue), denom),
#   )


ciPowerOut <- ciClean(ciPower)



```{r, Table1, include = TRUE, echo = FALSE, results = "asis"}
knitr::kable(cisOut, caption = "Table 1. Estimates of the number of published articles that use GPower", booktabs = T, linesep = "", align = "c") %>% 
  kable_styling(latex_options = "striped") %>%
  kable_styling(latex_options = c("hold_position")) %>%
  kable_styling(font_size = 8) %>% 
  add_footnote(paste0("The table is divided into articles that use G*Power for: any power calculation related to any statistical test (Any power calculation), a power calculation for any statistical test that solves for sample size (Sample size calculation), and a power calculation for an ANOVA that solves for sample size (ANOVA sample size calculation). The total number of articles in each database since 2017 is: PubMed Central ", prettyNum(pmcTotal, big.mark=",", scientific=F), "; PubMed ", prettyNum(pubmedTotal, big.mark=",", scientific=F), "."), notation = "none", threeparttable = T) 
```


-->
  
  <!--
  
  ## CODE to get preliminary results to the GPower team
  
  # filter for calculations solving for power
  d <- d %>% filter(grepl("Solves for sample size", powerCalcType))


reproducible <- table(d$reproducible)
power <- table(d$power)
alpha <- table(d$alpha)
effect_size_type <- table(d$alpha)
sample_size <- table(d$sample_size)
effectSizeValue <- table(d$effectSizeValue)
stat_test <- table(d$stat_test)
ANOVAwithinBetween <- table(d$ANOVAwithinBetween)
mult_compare <- table(d$mult_compare)
match <- table(d$match)
error <- table(d$error)



filepath <- paste0(getwd(), "/gpower_codingResolvingANOVAs_220929.csv" )
rawANOVA <- read.csv(filepath, header = TRUE)

dANOVA <- rawANOVA
# filter to only include resolved coding using with the keyword/charcter "/"
dANOVA <- dANOVA %>% filter(grepl("/", coder))
dANOVA <- dANOVA[1:9,]
ANOVAwithinBetweenANOVA <- table(dANOVA$ANOVAwithinBetween)
```

## prelim reproducibility results
Reproducible with assumptions: `r sum(grepl("Yes, but", d$reproducible))`

Reproducible without making assumptions: `r sum(grepl("Yes, based solely", d$reproducible))`

Not reproducible: `r sum(grepl("No", d$reproducible))`

Effect size value not reported: `r sum(grepl("No", d$effectSizeValue))`

Effect size type not reported: `r sum(grepl("No", d$effect_size_type))`

Statistical test not reported: `r sum(grepl("NOT reported", d$stat_test))`


```{r repro}

drep <- d %>% filter(grepl("Yes", reproducible))

reproducible_rep <- table(drep$reproducible)
power_rep <- table(drep$power)
alpha_rep <- table(drep$alpha)
effect_size_type_rep <- table(drep$effect_size_type)
sample_size_rep <- table(drep$sample_size)
effectSizeValue_rep <- table(drep$effectSizeValue)
stat_test_rep <- table(drep$stat_test)
ANOVAwithinBetween_rep <- table(drep$ANOVAwithinBetween)
mult_compare_rep <- table(drep$mult_compare)
match_rep <- table(drep$match)
error_rep <- table(drep$error)


```




-->
  
  

